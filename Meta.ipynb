{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.capacity = capacity\n",
    "        self.counter = 0\n",
    "        self.batch_size = 64\n",
    "        self.state_buffer = np.zeros((self.capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.capacity, num_states))\n",
    "\n",
    "    def insert(self, obs_tuple):\n",
    "        index = self.index\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.counter += 1\n",
    "    \n",
    "    def append(self, d):\n",
    "        for i in d:\n",
    "            self.insert(i)\n",
    "    \n",
    "    def sample(self):\n",
    "        idx = np.random.choice(self.size, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[idx])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[idx])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[idx])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[idx])\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch\n",
    "    \n",
    "    @property\n",
    "    def raw(self):\n",
    "        d = [(self.state_buffer[i], self.action_buffer[i], self.reward_buffer[i], self.next_state_buffer[i]) for i in range(self.size)]\n",
    "        return d\n",
    "            \n",
    "    @property\n",
    "    def size(self):\n",
    "        if self.counter >= self.capacity:\n",
    "            return self.capacity\n",
    "        else:\n",
    "            return self.counter\n",
    "    \n",
    "    @property\n",
    "    def index(self):\n",
    "        return self.counter % self.capacity\n",
    "    \n",
    "    @property\n",
    "    def batches(self):\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer)\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer)\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer)\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ddpg_update(d, actor, actor_optimizer, critic, critic_optimizer, update_critic=True):\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = d.sample()\n",
    "    \n",
    "    if update_critic:\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = critic([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actions = actor(state_batch, training=True)\n",
    "        critic_value = critic([state_batch, actions], training=True)\n",
    "        # Used `-value` as we want to maximize the value given\n",
    "        # by the critic for our actions\n",
    "        actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "    actor_grad = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def target_update(target, model):\n",
    "    target.variables = model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def teacher_update(actor, actor_optim, critic, critic_optim, d, meta):\n",
    "    state_batch, action_batch, reward_batch, _ = d.sample()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        reward = critic([state_batch, action_batch], training=True)\n",
    "        critic_loss = tf.math.reduce_mean(tf.math.square(reward - reward_batch))\n",
    "    \n",
    "    critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "    critic_optim.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        target_actions = actor(state_batch, training=True)\n",
    "        reward = critic([state_batch, target_actions], training=False)\n",
    "        print('reward', reward)\n",
    "        log_reward = tf.math.log(reward + 0.00001)\n",
    "        print('log_reward', log_reward)\n",
    "        actor_loss = tf.math.multiply(tf.cast(meta, dtype=\"float32\"), tf.reduce_sum(log_reward))\n",
    "        \n",
    "    actor_grad = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optim.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "    \n",
    "    return actor_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give: policy(model) and current state\n",
    "# return: next action\n",
    "def next_action(policy, state):\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    sampled_actions = tf.squeeze(policy(state))\n",
    "    sampled_actions = sampled_actions.numpy()\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n",
    "# sample n steps using policy\n",
    "# give: policy, n\n",
    "# return: [sars], accumulated reward\n",
    "def generate(policy, n):\n",
    "    prev_state = env.reset()\n",
    "    d = []\n",
    "    total_reward = 0\n",
    "    for i in range(n):\n",
    "        action = next_action(policy, prev_state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        d.append((prev_state, action, reward, state))\n",
    "        prev_state = state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return d, total_reward\n",
    "\n",
    "def copy(model):\n",
    "    new_model = tf.keras.models.clone_model(model)\n",
    "    new_model.set_weights(model.get_weights())\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "teacher_lr = 0.0001\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "\n",
    "total_iterations = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "d0_rollout = 100\n",
    "d1_rollout = 200\n",
    "pi_rollout = 50\n",
    "buffer_capacity = 50000\n",
    "log_interval = 50\n",
    "\n",
    "# To store reward history of each episode\n",
    "it_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "meta_reward_list = []\n",
    "teacher_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize pi\n",
    "teacher_actor = get_actor()\n",
    "teacher_critic = get_critic()\n",
    "\n",
    "# Step 1: Initialize pie\n",
    "actor = get_actor()\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "critic = get_critic()\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "# Making the weights equal initially\n",
    "teacher_actor.set_weights(actor.get_weights())\n",
    "teacher_actor_optim = tf.keras.optimizers.Adam(actor_lr)\n",
    "teacher_critic.set_weights(critic.get_weights())\n",
    "teacher_critic_optim = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "# Step 2:\n",
    "d0, d1 = Buffer(d0_rollout), Buffer(d1_rollout)\n",
    "data, meta_pi = generate(actor, d1_rollout)\n",
    "d1.append(data)\n",
    "\n",
    "# Step 3:\n",
    "buffer = Buffer(buffer_capacity)\n",
    "buffer.append(d1.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Meta Reward: 76.00502663142356\n",
      "reward tf.Tensor(\n",
      "[[-0.2075641 ]\n",
      " [-0.11310516]\n",
      " [-0.20884348]\n",
      " [-0.20286639]\n",
      " [-0.18154885]\n",
      " [-0.12940751]\n",
      " [-0.24385558]\n",
      " [-0.21137585]\n",
      " [-0.16603556]\n",
      " [-0.31080964]\n",
      " [-0.31080964]\n",
      " [-0.18062712]\n",
      " [-0.09631675]\n",
      " [-0.30195868]\n",
      " [-0.11808723]\n",
      " [-0.2896113 ]\n",
      " [-0.22084314]\n",
      " [-0.11045219]\n",
      " [-0.21036066]\n",
      " [-0.31420198]\n",
      " [-0.12481657]\n",
      " [-0.17174566]\n",
      " [-0.19001698]\n",
      " [-0.1918077 ]\n",
      " [-0.3042411 ]\n",
      " [-0.28510246]\n",
      " [-0.20884348]\n",
      " [-0.2090912 ]\n",
      " [-0.22084314]\n",
      " [-0.11955473]\n",
      " [-0.30195868]\n",
      " [-0.15749347]\n",
      " [-0.29673788]\n",
      " [-0.14488253]\n",
      " [-0.1474989 ]\n",
      " [-0.20884348]\n",
      " [-0.24385558]\n",
      " [-0.30597997]\n",
      " [-0.1918077 ]\n",
      " [-0.3049949 ]\n",
      " [-0.3049949 ]\n",
      " [-0.23121808]\n",
      " [-0.14444424]\n",
      " [-0.15884933]\n",
      " [-0.18004446]\n",
      " [-0.12940751]\n",
      " [-0.13937041]\n",
      " [-0.23121808]\n",
      " [-0.11955473]\n",
      " [-0.29842427]\n",
      " [-0.22084314]\n",
      " [-0.18499522]\n",
      " [-0.11808723]\n",
      " [-0.20295788]\n",
      " [-0.27096602]\n",
      " [-0.1393637 ]\n",
      " [-0.11360949]\n",
      " [-0.29842427]\n",
      " [-0.31420198]\n",
      " [-0.17174566]\n",
      " [-0.20484845]\n",
      " [-0.13937041]\n",
      " [-0.28059444]\n",
      " [-0.15264085]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -862.7263423311834\n",
      "Iteration 1\n",
      "Meta Reward: -106.85230916835894\n",
      "reward tf.Tensor(\n",
      "[[-0.1475459 ]\n",
      " [-0.6844092 ]\n",
      " [-0.81648034]\n",
      " [-0.7977801 ]\n",
      " [-0.7782219 ]\n",
      " [-0.2901362 ]\n",
      " [-0.16390482]\n",
      " [-0.7574691 ]\n",
      " [-0.66703206]\n",
      " [-0.36770502]\n",
      " [-0.32991913]\n",
      " [-0.32991913]\n",
      " [-0.34427065]\n",
      " [-0.7797161 ]\n",
      " [-0.82219326]\n",
      " [-0.84738076]\n",
      " [-0.5999405 ]\n",
      " [-0.4037775 ]\n",
      " [-0.51080006]\n",
      " [-0.60408455]\n",
      " [-0.47229958]\n",
      " [-0.2371115 ]\n",
      " [-0.60408455]\n",
      " [-0.6298969 ]\n",
      " [-0.7797161 ]\n",
      " [-0.8473599 ]\n",
      " [-0.51080006]\n",
      " [-0.936524  ]\n",
      " [-0.5650278 ]\n",
      " [-0.47229958]\n",
      " [-0.50462633]\n",
      " [-0.8473599 ]\n",
      " [-0.8475216 ]\n",
      " [-0.5999405 ]\n",
      " [-0.5780576 ]\n",
      " [-0.9187649 ]\n",
      " [-0.6298969 ]\n",
      " [-0.2286276 ]\n",
      " [-0.46503657]\n",
      " [-0.7797161 ]\n",
      " [-0.5726352 ]\n",
      " [-0.7782219 ]\n",
      " [-0.47229958]\n",
      " [-0.19182405]\n",
      " [-0.36770502]\n",
      " [-0.5650278 ]\n",
      " [-0.8932011 ]\n",
      " [-0.7977801 ]\n",
      " [-0.4232837 ]\n",
      " [-0.37785342]\n",
      " [-0.46503657]\n",
      " [-0.9110734 ]\n",
      " [-0.2403779 ]\n",
      " [-0.2125553 ]\n",
      " [-0.34446618]\n",
      " [-0.51080006]\n",
      " [-0.9110734 ]\n",
      " [-0.6610292 ]\n",
      " [-0.919259  ]\n",
      " [-0.8791957 ]\n",
      " [-0.29846463]\n",
      " [-0.27327132]\n",
      " [-0.6989023 ]\n",
      " [-0.7782219 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -746.8424031192969\n",
      "Iteration 2\n",
      "Meta Reward: -953.8691781765701\n",
      "reward tf.Tensor(\n",
      "[[-0.18185133]\n",
      " [-0.8052952 ]\n",
      " [-0.69992065]\n",
      " [-0.4573723 ]\n",
      " [-0.75804484]\n",
      " [-0.29186985]\n",
      " [-0.42454007]\n",
      " [-1.7377741 ]\n",
      " [-0.4573723 ]\n",
      " [-0.52599883]\n",
      " [-0.2984051 ]\n",
      " [-0.29713878]\n",
      " [-0.57412714]\n",
      " [-0.25885814]\n",
      " [-0.34972596]\n",
      " [-0.19114977]\n",
      " [-1.5392065 ]\n",
      " [-0.801693  ]\n",
      " [-0.69992065]\n",
      " [-0.66153145]\n",
      " [-0.22781925]\n",
      " [-0.8052952 ]\n",
      " [-0.5120605 ]\n",
      " [-0.5535143 ]\n",
      " [-0.947095  ]\n",
      " [-0.812291  ]\n",
      " [-1.1776297 ]\n",
      " [-1.4473249 ]\n",
      " [-0.8052952 ]\n",
      " [-0.40868565]\n",
      " [-1.2054656 ]\n",
      " [-0.38043955]\n",
      " [-0.21518016]\n",
      " [-1.1127528 ]\n",
      " [-0.57412714]\n",
      " [-1.3299305 ]\n",
      " [-0.34972596]\n",
      " [-0.57412714]\n",
      " [-0.6949502 ]\n",
      " [-0.49932906]\n",
      " [-0.5120605 ]\n",
      " [-0.2576402 ]\n",
      " [-0.812291  ]\n",
      " [-1.4473249 ]\n",
      " [-1.0029366 ]\n",
      " [-0.7288356 ]\n",
      " [-0.29713878]\n",
      " [-1.097568  ]\n",
      " [-0.40868565]\n",
      " [-1.1617733 ]\n",
      " [-1.2786524 ]\n",
      " [-1.9009552 ]\n",
      " [-0.18185133]\n",
      " [-1.6837085 ]\n",
      " [-0.66153145]\n",
      " [-0.18739228]\n",
      " [-0.24024309]\n",
      " [-1.841725  ]\n",
      " [-0.3932467 ]\n",
      " [-0.29186985]\n",
      " [-1.5392065 ]\n",
      " [-1.2873529 ]\n",
      " [-1.8857279 ]\n",
      " [-1.8857279 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -790.959417687075\n",
      "Iteration 3\n",
      "Meta Reward: -278.98027400613535\n",
      "reward tf.Tensor(\n",
      "[[-0.4866458 ]\n",
      " [-0.637762  ]\n",
      " [-0.5212314 ]\n",
      " [-0.6688738 ]\n",
      " [-0.8710942 ]\n",
      " [-0.5849666 ]\n",
      " [-0.39337733]\n",
      " [-0.47828242]\n",
      " [-0.460344  ]\n",
      " [-0.69486314]\n",
      " [-0.3564136 ]\n",
      " [-0.4809423 ]\n",
      " [-0.8818076 ]\n",
      " [-0.8588746 ]\n",
      " [-0.5771038 ]\n",
      " [-0.8029497 ]\n",
      " [-0.34779486]\n",
      " [-0.8263181 ]\n",
      " [-0.54443437]\n",
      " [-0.6111369 ]\n",
      " [-0.4954204 ]\n",
      " [-0.69408286]\n",
      " [-0.8818076 ]\n",
      " [-0.57797116]\n",
      " [-0.6597782 ]\n",
      " [-0.66379017]\n",
      " [-0.708605  ]\n",
      " [-0.6213584 ]\n",
      " [-0.5482393 ]\n",
      " [-0.69486314]\n",
      " [-0.54443437]\n",
      " [-0.35469165]\n",
      " [-0.44350532]\n",
      " [-0.54443437]\n",
      " [-0.7110478 ]\n",
      " [-0.44350532]\n",
      " [-0.738684  ]\n",
      " [-0.4954204 ]\n",
      " [-0.5212314 ]\n",
      " [-0.71112275]\n",
      " [-0.39337733]\n",
      " [-0.48286563]\n",
      " [-0.5882924 ]\n",
      " [-0.6213584 ]\n",
      " [-0.41895497]\n",
      " [-0.73056346]\n",
      " [-0.85730654]\n",
      " [-0.56448215]\n",
      " [-0.79395014]\n",
      " [-0.6213584 ]\n",
      " [-0.5882924 ]\n",
      " [-0.660992  ]\n",
      " [-0.41771147]\n",
      " [-0.80780905]\n",
      " [-0.80996466]\n",
      " [-0.6688738 ]\n",
      " [-0.87773514]\n",
      " [-0.4829569 ]\n",
      " [-0.6275199 ]\n",
      " [-0.5065214 ]\n",
      " [-0.6213584 ]\n",
      " [-0.40647742]\n",
      " [-0.8588746 ]\n",
      " [-0.5482393 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -850.1015994214569\n",
      "Iteration 4\n",
      "Meta Reward: -371.07499566414117\n",
      "reward tf.Tensor(\n",
      "[[-1.214101  ]\n",
      " [-0.9164238 ]\n",
      " [-1.7694184 ]\n",
      " [-0.50413287]\n",
      " [-0.502025  ]\n",
      " [-0.4609085 ]\n",
      " [-0.9027586 ]\n",
      " [-2.2856631 ]\n",
      " [-0.59450454]\n",
      " [-0.5085036 ]\n",
      " [-1.1595238 ]\n",
      " [-0.8450882 ]\n",
      " [-2.0647876 ]\n",
      " [-1.4685928 ]\n",
      " [-0.48840618]\n",
      " [-0.4281647 ]\n",
      " [-1.7111231 ]\n",
      " [-1.4373188 ]\n",
      " [-1.5261855 ]\n",
      " [-1.0210036 ]\n",
      " [-1.6258241 ]\n",
      " [-2.1278267 ]\n",
      " [-2.0786269 ]\n",
      " [-0.48840618]\n",
      " [-1.3206012 ]\n",
      " [-1.2258521 ]\n",
      " [-0.8453393 ]\n",
      " [-2.0647876 ]\n",
      " [-1.214101  ]\n",
      " [-0.59450454]\n",
      " [-1.4638981 ]\n",
      " [-1.4373188 ]\n",
      " [-1.6838096 ]\n",
      " [-1.0326622 ]\n",
      " [-2.0647876 ]\n",
      " [-1.1636053 ]\n",
      " [-2.2118886 ]\n",
      " [-0.52077246]\n",
      " [-0.7374494 ]\n",
      " [-0.9027586 ]\n",
      " [-0.49643645]\n",
      " [-1.6200649 ]\n",
      " [-2.093662  ]\n",
      " [-0.48840618]\n",
      " [-2.2190404 ]\n",
      " [-1.6266274 ]\n",
      " [-1.6603289 ]\n",
      " [-1.5839471 ]\n",
      " [-1.625382  ]\n",
      " [-2.1278267 ]\n",
      " [-1.4685928 ]\n",
      " [-1.6908065 ]\n",
      " [-0.49979615]\n",
      " [-1.4638981 ]\n",
      " [-1.386787  ]\n",
      " [-0.95346534]\n",
      " [-1.6838096 ]\n",
      " [-0.9164238 ]\n",
      " [-0.587359  ]\n",
      " [-0.5888796 ]\n",
      " [-2.233181  ]\n",
      " [-1.6266274 ]\n",
      " [-1.4685928 ]\n",
      " [-2.233181  ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1046.2324549192112\n",
      "Iteration 5\n",
      "Meta Reward: 1050.3765821124953\n",
      "reward tf.Tensor(\n",
      "[[-4.4111576 ]\n",
      " [-0.6715174 ]\n",
      " [-0.55852205]\n",
      " [-0.45049056]\n",
      " [-0.801941  ]\n",
      " [-1.3055713 ]\n",
      " [-0.6412055 ]\n",
      " [-3.1937957 ]\n",
      " [-3.5763104 ]\n",
      " [-2.5226998 ]\n",
      " [-0.45049056]\n",
      " [-4.218805  ]\n",
      " [-3.5959635 ]\n",
      " [-1.2641671 ]\n",
      " [-0.9908884 ]\n",
      " [-2.8006978 ]\n",
      " [-3.358271  ]\n",
      " [-2.5226998 ]\n",
      " [-3.9243622 ]\n",
      " [-2.0430422 ]\n",
      " [-2.585091  ]\n",
      " [-3.1285536 ]\n",
      " [-0.4028538 ]\n",
      " [-1.2641671 ]\n",
      " [-3.5763104 ]\n",
      " [-0.8706259 ]\n",
      " [-1.2736083 ]\n",
      " [-2.8032308 ]\n",
      " [-0.7249591 ]\n",
      " [-1.0590125 ]\n",
      " [-0.59743696]\n",
      " [-0.79657024]\n",
      " [-0.6949924 ]\n",
      " [-0.42314565]\n",
      " [-2.7374582 ]\n",
      " [-2.372085  ]\n",
      " [-0.5209699 ]\n",
      " [-2.0864024 ]\n",
      " [-1.6466186 ]\n",
      " [-1.465004  ]\n",
      " [-4.457137  ]\n",
      " [-0.7691547 ]\n",
      " [-4.3375144 ]\n",
      " [-0.40881875]\n",
      " [-3.9124653 ]\n",
      " [-2.0864024 ]\n",
      " [-1.1039609 ]\n",
      " [-4.2324142 ]\n",
      " [-0.7691547 ]\n",
      " [-1.1129515 ]\n",
      " [-2.5226998 ]\n",
      " [-0.7691547 ]\n",
      " [-2.0430422 ]\n",
      " [-0.72010857]\n",
      " [-1.1129515 ]\n",
      " [-3.5306635 ]\n",
      " [-0.801941  ]\n",
      " [-1.0590125 ]\n",
      " [-4.077403  ]\n",
      " [-2.9424608 ]\n",
      " [-1.0590125 ]\n",
      " [-1.4420792 ]\n",
      " [-3.358271  ]\n",
      " [-0.79657024]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: -1030.543143635523\n",
      "Iteration 6\n",
      "Meta Reward: -764.0149343035616\n",
      "reward tf.Tensor(\n",
      "[[-0.9015813 ]\n",
      " [-2.9872649 ]\n",
      " [-1.1763985 ]\n",
      " [-2.0181777 ]\n",
      " [-3.37118   ]\n",
      " [-3.655759  ]\n",
      " [-2.1199176 ]\n",
      " [-2.07351   ]\n",
      " [-4.266158  ]\n",
      " [-2.0181777 ]\n",
      " [-4.344458  ]\n",
      " [-1.4485459 ]\n",
      " [-3.37118   ]\n",
      " [-2.9602106 ]\n",
      " [-3.9919453 ]\n",
      " [-3.6492403 ]\n",
      " [-2.3864415 ]\n",
      " [-3.37118   ]\n",
      " [-0.8273794 ]\n",
      " [-1.2437906 ]\n",
      " [-3.37118   ]\n",
      " [-1.5641947 ]\n",
      " [-0.5569063 ]\n",
      " [-2.9872649 ]\n",
      " [-0.8273794 ]\n",
      " [-2.041559  ]\n",
      " [-3.0486414 ]\n",
      " [-4.0620613 ]\n",
      " [-2.497628  ]\n",
      " [-2.874389  ]\n",
      " [-0.5801077 ]\n",
      " [-3.6492403 ]\n",
      " [-1.1221223 ]\n",
      " [-3.018285  ]\n",
      " [-2.927667  ]\n",
      " [-2.8608367 ]\n",
      " [-2.9556344 ]\n",
      " [-1.1630114 ]\n",
      " [-3.018285  ]\n",
      " [-2.774782  ]\n",
      " [-2.07351   ]\n",
      " [-2.1969802 ]\n",
      " [-3.8551905 ]\n",
      " [-2.5247447 ]\n",
      " [-0.8273794 ]\n",
      " [-0.88723576]\n",
      " [-0.98547447]\n",
      " [-2.0666342 ]\n",
      " [-3.6492403 ]\n",
      " [-1.4723651 ]\n",
      " [-3.2386544 ]\n",
      " [-3.3804185 ]\n",
      " [-1.1763985 ]\n",
      " [-3.762818  ]\n",
      " [-3.4656985 ]\n",
      " [-3.9986243 ]\n",
      " [-1.5641947 ]\n",
      " [-0.6986378 ]\n",
      " [-1.032074  ]\n",
      " [-2.774782  ]\n",
      " [-0.5801077 ]\n",
      " [-0.65547687]\n",
      " [-0.65547687]\n",
      " [-1.1763985 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1096.2384168110596\n",
      "Iteration 7\n",
      "Meta Reward: 303.36172354283053\n",
      "reward tf.Tensor(\n",
      "[[-1.1457046 ]\n",
      " [-2.7926066 ]\n",
      " [-0.8588367 ]\n",
      " [-2.7227478 ]\n",
      " [-1.286892  ]\n",
      " [-2.2301662 ]\n",
      " [-0.7757979 ]\n",
      " [-0.9557536 ]\n",
      " [-2.4273202 ]\n",
      " [-4.0739737 ]\n",
      " [-2.4561725 ]\n",
      " [-2.8465652 ]\n",
      " [-2.070668  ]\n",
      " [-2.1451313 ]\n",
      " [-0.9769347 ]\n",
      " [-1.6017505 ]\n",
      " [-2.095508  ]\n",
      " [-2.095508  ]\n",
      " [-1.0846492 ]\n",
      " [-2.4561725 ]\n",
      " [-2.6419575 ]\n",
      " [-2.7158263 ]\n",
      " [-2.794033  ]\n",
      " [-1.9188008 ]\n",
      " [-2.7926066 ]\n",
      " [-0.8199416 ]\n",
      " [-1.233106  ]\n",
      " [-2.5143285 ]\n",
      " [-1.087356  ]\n",
      " [-2.6683943 ]\n",
      " [-2.240145  ]\n",
      " [-2.095508  ]\n",
      " [-0.7759412 ]\n",
      " [-2.6181438 ]\n",
      " [-3.6547935 ]\n",
      " [-3.7626839 ]\n",
      " [-2.8725522 ]\n",
      " [-2.1451313 ]\n",
      " [-1.4395941 ]\n",
      " [-2.7741716 ]\n",
      " [-3.9830446 ]\n",
      " [-2.4561725 ]\n",
      " [-0.8100841 ]\n",
      " [-3.7626839 ]\n",
      " [-2.7741716 ]\n",
      " [-0.8588367 ]\n",
      " [-0.9557536 ]\n",
      " [-3.5389287 ]\n",
      " [-0.8588367 ]\n",
      " [-0.7759412 ]\n",
      " [-2.3289754 ]\n",
      " [-3.5389287 ]\n",
      " [-3.889461  ]\n",
      " [-2.7978008 ]\n",
      " [-1.9547428 ]\n",
      " [-2.4273202 ]\n",
      " [-1.233106  ]\n",
      " [-3.4391353 ]\n",
      " [-0.90385544]\n",
      " [-0.9769347 ]\n",
      " [-1.9232789 ]\n",
      " [-1.8218964 ]\n",
      " [-1.4395941 ]\n",
      " [-2.4688985 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1054.6245080071715\n",
      "Iteration 8\n",
      "Meta Reward: -350.96258953043923\n",
      "reward tf.Tensor(\n",
      "[[-1.6338737]\n",
      " [-1.1141933]\n",
      " [-1.5124652]\n",
      " [-1.1011596]\n",
      " [-1.3078084]\n",
      " [-1.100826 ]\n",
      " [-1.4268531]\n",
      " [-1.5518246]\n",
      " [-1.4324179]\n",
      " [-1.3269844]\n",
      " [-1.3532822]\n",
      " [-1.0714123]\n",
      " [-1.4268531]\n",
      " [-1.203195 ]\n",
      " [-1.4268531]\n",
      " [-1.4798174]\n",
      " [-1.5518246]\n",
      " [-1.7465013]\n",
      " [-0.9177988]\n",
      " [-1.3826271]\n",
      " [-1.0911818]\n",
      " [-1.5048518]\n",
      " [-1.2865062]\n",
      " [-0.9005733]\n",
      " [-1.3557723]\n",
      " [-1.2059696]\n",
      " [-1.1141933]\n",
      " [-1.3826271]\n",
      " [-1.4853071]\n",
      " [-1.7669659]\n",
      " [-1.7769067]\n",
      " [-1.3078084]\n",
      " [-1.4236193]\n",
      " [-1.364083 ]\n",
      " [-1.5048518]\n",
      " [-1.3532822]\n",
      " [-1.4236193]\n",
      " [-1.4853071]\n",
      " [-1.3557723]\n",
      " [-1.9099566]\n",
      " [-1.05384  ]\n",
      " [-1.928298 ]\n",
      " [-1.7059051]\n",
      " [-1.9154224]\n",
      " [-1.408543 ]\n",
      " [-1.659092 ]\n",
      " [-1.4370244]\n",
      " [-1.2769815]\n",
      " [-1.0186801]\n",
      " [-1.9099566]\n",
      " [-1.8311777]\n",
      " [-1.1715539]\n",
      " [-1.3337821]\n",
      " [-1.8311777]\n",
      " [-1.6543989]\n",
      " [-1.9022081]\n",
      " [-1.6338737]\n",
      " [-1.5064577]\n",
      " [-1.8311777]\n",
      " [-1.364083 ]\n",
      " [-1.2285123]\n",
      " [-1.7897897]\n",
      " [-1.3078084]\n",
      " [-0.9251306]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1066.972068782763\n",
      "Iteration 9\n",
      "Meta Reward: 158.66659205911992\n",
      "reward tf.Tensor(\n",
      "[[-1.4163435]\n",
      " [-3.3574123]\n",
      " [-3.9240189]\n",
      " [-5.2723145]\n",
      " [-2.1516285]\n",
      " [-3.0310762]\n",
      " [-4.5322995]\n",
      " [-4.2052727]\n",
      " [-2.7017238]\n",
      " [-2.2077763]\n",
      " [-1.9412194]\n",
      " [-5.369075 ]\n",
      " [-4.1079483]\n",
      " [-3.2695813]\n",
      " [-2.1392546]\n",
      " [-2.7017238]\n",
      " [-2.8907466]\n",
      " [-1.5603309]\n",
      " [-1.5128111]\n",
      " [-1.9412194]\n",
      " [-2.1392546]\n",
      " [-1.7791125]\n",
      " [-1.1928664]\n",
      " [-1.9564306]\n",
      " [-5.381374 ]\n",
      " [-1.724175 ]\n",
      " [-4.928626 ]\n",
      " [-2.4709754]\n",
      " [-4.138955 ]\n",
      " [-2.4709754]\n",
      " [-1.9412194]\n",
      " [-4.2052727]\n",
      " [-3.4116745]\n",
      " [-1.6650672]\n",
      " [-5.4485474]\n",
      " [-4.2758174]\n",
      " [-5.668975 ]\n",
      " [-5.2723145]\n",
      " [-3.7326415]\n",
      " [-2.4284658]\n",
      " [-4.310934 ]\n",
      " [-3.7326415]\n",
      " [-1.1568497]\n",
      " [-1.1934413]\n",
      " [-5.9295654]\n",
      " [-6.0063534]\n",
      " [-1.5603309]\n",
      " [-3.9240189]\n",
      " [-3.4116745]\n",
      " [-4.870182 ]\n",
      " [-5.369075 ]\n",
      " [-4.9591074]\n",
      " [-5.7695246]\n",
      " [-2.4709754]\n",
      " [-3.4116745]\n",
      " [-2.0020626]\n",
      " [-1.4163435]\n",
      " [-5.4189515]\n",
      " [-4.388671 ]\n",
      " [-3.3574123]\n",
      " [-1.2560875]\n",
      " [-2.9382772]\n",
      " [-3.3574123]\n",
      " [-3.615531 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1116.3435685335153\n",
      "Iteration 10\n",
      "Meta Reward: 254.74342112261593\n",
      "reward tf.Tensor(\n",
      "[[-2.1605198]\n",
      " [-1.6041292]\n",
      " [-2.3335154]\n",
      " [-1.5464487]\n",
      " [-1.9823796]\n",
      " [-1.7946519]\n",
      " [-1.4689778]\n",
      " [-2.0364344]\n",
      " [-3.0150275]\n",
      " [-3.1332057]\n",
      " [-1.8517698]\n",
      " [-2.3601167]\n",
      " [-2.3050063]\n",
      " [-1.5464487]\n",
      " [-2.9705522]\n",
      " [-3.2113237]\n",
      " [-3.1021843]\n",
      " [-1.8967434]\n",
      " [-1.4689778]\n",
      " [-2.799547 ]\n",
      " [-2.3077395]\n",
      " [-2.1707084]\n",
      " [-1.8517698]\n",
      " [-1.478573 ]\n",
      " [-2.9949377]\n",
      " [-2.3594854]\n",
      " [-1.2371188]\n",
      " [-2.2028852]\n",
      " [-3.0642173]\n",
      " [-1.4955627]\n",
      " [-2.0829525]\n",
      " [-2.12837  ]\n",
      " [-2.3601167]\n",
      " [-2.2001388]\n",
      " [-3.133026 ]\n",
      " [-1.8517698]\n",
      " [-2.4812634]\n",
      " [-1.2972746]\n",
      " [-2.280575 ]\n",
      " [-1.6842004]\n",
      " [-1.8231375]\n",
      " [-1.6842004]\n",
      " [-2.9949377]\n",
      " [-2.7529697]\n",
      " [-1.275949 ]\n",
      " [-1.2881078]\n",
      " [-3.1715982]\n",
      " [-1.275949 ]\n",
      " [-1.6855527]\n",
      " [-1.2881078]\n",
      " [-2.1707084]\n",
      " [-1.8164312]\n",
      " [-2.75968  ]\n",
      " [-1.5504323]\n",
      " [-2.2552936]\n",
      " [-1.8761739]\n",
      " [-1.9823796]\n",
      " [-2.1605198]\n",
      " [-1.4689778]\n",
      " [-3.2113237]\n",
      " [-3.1021843]\n",
      " [-2.3601167]\n",
      " [-2.2552936]\n",
      " [-1.8329014]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1157.1411208195534\n",
      "Iteration 11\n",
      "Meta Reward: 692.1615362177351\n",
      "reward tf.Tensor(\n",
      "[[-5.601565 ]\n",
      " [-6.446862 ]\n",
      " [-3.855133 ]\n",
      " [-9.347867 ]\n",
      " [-1.8684397]\n",
      " [-6.446862 ]\n",
      " [-6.7912683]\n",
      " [-4.559294 ]\n",
      " [-5.3068995]\n",
      " [-2.9135916]\n",
      " [-1.7118976]\n",
      " [-4.1240106]\n",
      " [-1.3038715]\n",
      " [-6.5679855]\n",
      " [-5.5241704]\n",
      " [-6.8665185]\n",
      " [-2.9161627]\n",
      " [-6.0217934]\n",
      " [-1.561051 ]\n",
      " [-7.65208  ]\n",
      " [-6.8665185]\n",
      " [-3.447377 ]\n",
      " [-7.943406 ]\n",
      " [-5.566992 ]\n",
      " [-9.347867 ]\n",
      " [-1.975463 ]\n",
      " [-5.601565 ]\n",
      " [-6.6045814]\n",
      " [-1.5875406]\n",
      " [-6.0217934]\n",
      " [-1.561051 ]\n",
      " [-4.789557 ]\n",
      " [-6.5598187]\n",
      " [-7.943406 ]\n",
      " [-1.7116653]\n",
      " [-1.3042083]\n",
      " [-1.8541185]\n",
      " [-5.3068995]\n",
      " [-6.5679855]\n",
      " [-5.8848023]\n",
      " [-9.347867 ]\n",
      " [-1.5875406]\n",
      " [-1.957572 ]\n",
      " [-4.3255215]\n",
      " [-6.456822 ]\n",
      " [-4.2520537]\n",
      " [-6.2937727]\n",
      " [-2.1525743]\n",
      " [-8.649693 ]\n",
      " [-6.2937727]\n",
      " [-2.3198955]\n",
      " [-1.6869912]\n",
      " [-6.259059 ]\n",
      " [-3.5793798]\n",
      " [-6.394087 ]\n",
      " [-6.0217934]\n",
      " [-4.9711947]\n",
      " [-2.9100146]\n",
      " [-6.5679855]\n",
      " [-6.8665185]\n",
      " [-1.8541185]\n",
      " [-6.7912683]\n",
      " [-4.892835 ]\n",
      " [-5.515613 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: -1189.390129018585\n",
      "Iteration 12\n",
      "Meta Reward: -120.72431032787767\n",
      "reward tf.Tensor(\n",
      "[[ -2.1500673]\n",
      " [ -2.5570986]\n",
      " [ -1.6480665]\n",
      " [ -7.0740957]\n",
      " [ -2.2947137]\n",
      " [ -2.912331 ]\n",
      " [ -5.681974 ]\n",
      " [ -7.095216 ]\n",
      " [ -5.681974 ]\n",
      " [ -4.5651493]\n",
      " [ -1.6988647]\n",
      " [ -1.5472713]\n",
      " [ -2.7816575]\n",
      " [ -1.6016569]\n",
      " [ -3.2959216]\n",
      " [ -1.8602016]\n",
      " [ -5.0504365]\n",
      " [ -9.257287 ]\n",
      " [ -2.5933692]\n",
      " [ -1.6016569]\n",
      " [-11.6662855]\n",
      " [ -7.637065 ]\n",
      " [ -1.979844 ]\n",
      " [ -1.704043 ]\n",
      " [ -5.9325457]\n",
      " [ -1.396209 ]\n",
      " [-12.015427 ]\n",
      " [ -4.9632444]\n",
      " [ -1.405391 ]\n",
      " [ -2.912331 ]\n",
      " [ -4.9632444]\n",
      " [-12.015427 ]\n",
      " [ -2.7919424]\n",
      " [ -5.681974 ]\n",
      " [ -3.0289524]\n",
      " [ -8.183685 ]\n",
      " [ -1.6988647]\n",
      " [ -1.1869029]\n",
      " [-11.130953 ]\n",
      " [ -7.8756332]\n",
      " [ -1.4604696]\n",
      " [ -1.1869029]\n",
      " [ -3.5750206]\n",
      " [ -2.4741585]\n",
      " [ -2.2947137]\n",
      " [ -1.396209 ]\n",
      " [-11.833657 ]\n",
      " [ -4.112149 ]\n",
      " [ -7.637065 ]\n",
      " [ -2.4741585]\n",
      " [-11.278515 ]\n",
      " [ -2.1840851]\n",
      " [ -1.396209 ]\n",
      " [ -2.2947137]\n",
      " [ -2.4741585]\n",
      " [ -4.9632444]\n",
      " [-10.262592 ]\n",
      " [ -4.9632444]\n",
      " [ -9.312933 ]\n",
      " [ -9.257287 ]\n",
      " [ -7.9962196]\n",
      " [ -9.257287 ]\n",
      " [ -2.7919424]\n",
      " [ -2.7816575]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1179.0193748783809\n",
      "Iteration 13\n",
      "Meta Reward: -786.3110139997843\n",
      "reward tf.Tensor(\n",
      "[[-9.069772 ]\n",
      " [-2.6140528]\n",
      " [-6.5829105]\n",
      " [-6.7409143]\n",
      " [-8.302212 ]\n",
      " [-4.579126 ]\n",
      " [-9.838242 ]\n",
      " [-4.212966 ]\n",
      " [-1.7330266]\n",
      " [-2.110523 ]\n",
      " [-9.838242 ]\n",
      " [-8.799828 ]\n",
      " [-1.9548489]\n",
      " [-4.8569217]\n",
      " [-8.302212 ]\n",
      " [-6.207434 ]\n",
      " [-6.7409143]\n",
      " [-2.110523 ]\n",
      " [-7.151492 ]\n",
      " [-2.130092 ]\n",
      " [-4.8569217]\n",
      " [-6.1697807]\n",
      " [-1.7330266]\n",
      " [-1.9534944]\n",
      " [-2.374737 ]\n",
      " [-1.7637227]\n",
      " [-9.510237 ]\n",
      " [-3.692919 ]\n",
      " [-3.426122 ]\n",
      " [-7.2964253]\n",
      " [-7.0373354]\n",
      " [-7.2964253]\n",
      " [-7.340991 ]\n",
      " [-2.362346 ]\n",
      " [-7.115623 ]\n",
      " [-6.653619 ]\n",
      " [-2.4788322]\n",
      " [-3.687671 ]\n",
      " [-1.9548489]\n",
      " [-2.1076164]\n",
      " [-6.0157547]\n",
      " [-9.467627 ]\n",
      " [-2.362346 ]\n",
      " [-1.7863474]\n",
      " [-7.0802207]\n",
      " [-1.9344068]\n",
      " [-5.4697027]\n",
      " [-8.022261 ]\n",
      " [-3.9031384]\n",
      " [-4.6534534]\n",
      " [-7.2964253]\n",
      " [-9.710941 ]\n",
      " [-1.692725 ]\n",
      " [-4.212966 ]\n",
      " [-3.692919 ]\n",
      " [-7.2440934]\n",
      " [-2.2163322]\n",
      " [-7.914953 ]\n",
      " [-7.151492 ]\n",
      " [-7.2964253]\n",
      " [-8.302212 ]\n",
      " [-1.7330266]\n",
      " [-8.799828 ]\n",
      " [-2.279715 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1160.4110200725506\n",
      "Iteration 14\n",
      "Meta Reward: -794.1140326975581\n",
      "reward tf.Tensor(\n",
      "[[-4.6662774]\n",
      " [-2.4933228]\n",
      " [-2.1501307]\n",
      " [-2.3090072]\n",
      " [-2.3891582]\n",
      " [-3.0700288]\n",
      " [-3.2980928]\n",
      " [-5.6765347]\n",
      " [-4.1233754]\n",
      " [-4.0071516]\n",
      " [-5.505676 ]\n",
      " [-4.0071516]\n",
      " [-5.824422 ]\n",
      " [-4.0071516]\n",
      " [-6.119352 ]\n",
      " [-2.7479572]\n",
      " [-4.544073 ]\n",
      " [-2.3248405]\n",
      " [-5.9463434]\n",
      " [-4.3381014]\n",
      " [-4.0690327]\n",
      " [-2.017283 ]\n",
      " [-5.9463434]\n",
      " [-5.7386217]\n",
      " [-2.888527 ]\n",
      " [-5.973424 ]\n",
      " [-2.8201504]\n",
      " [-2.3248405]\n",
      " [-4.6662774]\n",
      " [-4.568703 ]\n",
      " [-5.31984  ]\n",
      " [-2.3752599]\n",
      " [-4.463044 ]\n",
      " [-2.4933228]\n",
      " [-3.7948809]\n",
      " [-5.6765347]\n",
      " [-4.3800664]\n",
      " [-3.7948809]\n",
      " [-5.964381 ]\n",
      " [-3.7948809]\n",
      " [-3.0700288]\n",
      " [-5.959898 ]\n",
      " [-2.1501307]\n",
      " [-2.8201504]\n",
      " [-3.058724 ]\n",
      " [-4.1017027]\n",
      " [-5.505676 ]\n",
      " [-2.5611906]\n",
      " [-3.3961205]\n",
      " [-4.8228283]\n",
      " [-4.69563  ]\n",
      " [-2.4933228]\n",
      " [-3.7948809]\n",
      " [-4.6208973]\n",
      " [-6.1358724]\n",
      " [-4.5528164]\n",
      " [-2.3258324]\n",
      " [-4.3800664]\n",
      " [-4.7451515]\n",
      " [-6.1358724]\n",
      " [-3.6431866]\n",
      " [-4.0071516]\n",
      " [-4.7451515]\n",
      " [-4.1233754]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1183.237678815854\n",
      "Iteration 15\n",
      "Meta Reward: 663.4753931556702\n",
      "reward tf.Tensor(\n",
      "[[-2.4550161]\n",
      " [-6.4386253]\n",
      " [-3.8795016]\n",
      " [-5.5871577]\n",
      " [-4.2894464]\n",
      " [-2.4446735]\n",
      " [-7.2223105]\n",
      " [-3.6951404]\n",
      " [-6.1552067]\n",
      " [-3.0526652]\n",
      " [-6.246398 ]\n",
      " [-3.1645372]\n",
      " [-3.1645372]\n",
      " [-6.2300653]\n",
      " [-3.8408425]\n",
      " [-6.103836 ]\n",
      " [-5.45404  ]\n",
      " [-5.823471 ]\n",
      " [-7.557717 ]\n",
      " [-5.2591105]\n",
      " [-5.823471 ]\n",
      " [-3.8408425]\n",
      " [-4.40331  ]\n",
      " [-6.6635385]\n",
      " [-6.4386253]\n",
      " [-4.601749 ]\n",
      " [-8.393445 ]\n",
      " [-8.393445 ]\n",
      " [-5.849252 ]\n",
      " [-3.2099867]\n",
      " [-3.4798021]\n",
      " [-2.9729314]\n",
      " [-6.567197 ]\n",
      " [-7.7543054]\n",
      " [-5.854267 ]\n",
      " [-2.86375  ]\n",
      " [-8.282793 ]\n",
      " [-2.9013097]\n",
      " [-3.9887319]\n",
      " [-2.6346676]\n",
      " [-2.86375  ]\n",
      " [-2.3044157]\n",
      " [-5.7799873]\n",
      " [-2.742488 ]\n",
      " [-5.159486 ]\n",
      " [-5.7799873]\n",
      " [-5.854267 ]\n",
      " [-4.40331  ]\n",
      " [-2.2958133]\n",
      " [-4.1841702]\n",
      " [-6.2300653]\n",
      " [-8.617638 ]\n",
      " [-5.8670425]\n",
      " [-6.271786 ]\n",
      " [-6.5903597]\n",
      " [-6.4431767]\n",
      " [-5.2591105]\n",
      " [-5.5871577]\n",
      " [-8.357965 ]\n",
      " [-6.2300653]\n",
      " [-6.1805964]\n",
      " [-4.601749 ]\n",
      " [-6.2300653]\n",
      " [-4.601749 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1159.777953907498\n",
      "Iteration 16\n",
      "Meta Reward: -483.67621119439764\n",
      "reward tf.Tensor(\n",
      "[[ -3.6962605]\n",
      " [ -6.424142 ]\n",
      " [ -9.418773 ]\n",
      " [ -2.5645192]\n",
      " [ -8.888814 ]\n",
      " [-10.741185 ]\n",
      " [ -4.244815 ]\n",
      " [ -2.5487187]\n",
      " [ -2.7649074]\n",
      " [ -5.648557 ]\n",
      " [-10.047435 ]\n",
      " [ -2.9371371]\n",
      " [-10.167921 ]\n",
      " [ -2.5487187]\n",
      " [-10.824543 ]\n",
      " [ -5.54444  ]\n",
      " [ -2.5864131]\n",
      " [ -8.521335 ]\n",
      " [ -3.3136   ]\n",
      " [ -7.5099463]\n",
      " [ -3.4370584]\n",
      " [ -8.077095 ]\n",
      " [ -5.397512 ]\n",
      " [ -3.4370584]\n",
      " [ -4.5325856]\n",
      " [ -8.077095 ]\n",
      " [ -5.397512 ]\n",
      " [ -8.646619 ]\n",
      " [ -3.8676884]\n",
      " [-10.574866 ]\n",
      " [ -3.3136   ]\n",
      " [ -8.436131 ]\n",
      " [ -7.498924 ]\n",
      " [ -4.019298 ]\n",
      " [ -8.077095 ]\n",
      " [ -8.134854 ]\n",
      " [ -6.6681414]\n",
      " [ -8.077095 ]\n",
      " [ -3.6962605]\n",
      " [ -7.2433863]\n",
      " [ -3.8676884]\n",
      " [ -5.397512 ]\n",
      " [ -2.9774294]\n",
      " [-10.824543 ]\n",
      " [ -6.43735  ]\n",
      " [ -7.2433863]\n",
      " [ -7.2910976]\n",
      " [ -8.888814 ]\n",
      " [-11.022177 ]\n",
      " [ -7.689446 ]\n",
      " [ -8.134854 ]\n",
      " [-10.057066 ]\n",
      " [ -3.677438 ]\n",
      " [-10.057066 ]\n",
      " [ -4.6017365]\n",
      " [-10.057066 ]\n",
      " [ -7.2433863]\n",
      " [ -3.0938325]\n",
      " [ -9.218152 ]\n",
      " [ -3.1936972]\n",
      " [ -5.5826926]\n",
      " [ -3.677438 ]\n",
      " [ -2.5864131]\n",
      " [-10.184227 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1170.777617426755\n",
      "Iteration 17\n",
      "Meta Reward: 622.1173971104819\n",
      "reward tf.Tensor(\n",
      "[[ -9.858781 ]\n",
      " [ -8.13488  ]\n",
      " [ -5.2152476]\n",
      " [ -6.692541 ]\n",
      " [ -7.1763334]\n",
      " [ -6.6258097]\n",
      " [ -7.599608 ]\n",
      " [ -2.7418923]\n",
      " [ -6.196326 ]\n",
      " [ -8.136099 ]\n",
      " [ -9.481537 ]\n",
      " [-12.088243 ]\n",
      " [ -2.9631894]\n",
      " [ -6.325221 ]\n",
      " [-11.198097 ]\n",
      " [-10.182603 ]\n",
      " [ -6.77132  ]\n",
      " [ -4.2159724]\n",
      " [ -8.784991 ]\n",
      " [ -6.692541 ]\n",
      " [ -7.460388 ]\n",
      " [-12.370827 ]\n",
      " [-12.370827 ]\n",
      " [ -9.910617 ]\n",
      " [ -4.0944667]\n",
      " [ -5.705462 ]\n",
      " [ -7.79253  ]\n",
      " [-10.089069 ]\n",
      " [ -9.401613 ]\n",
      " [ -5.4108615]\n",
      " [-12.183059 ]\n",
      " [ -7.051433 ]\n",
      " [ -3.7372928]\n",
      " [ -4.2159724]\n",
      " [ -7.599608 ]\n",
      " [-11.992079 ]\n",
      " [ -5.705462 ]\n",
      " [ -8.136099 ]\n",
      " [ -3.6546881]\n",
      " [ -7.051433 ]\n",
      " [ -3.060538 ]\n",
      " [-11.883553 ]\n",
      " [-11.992079 ]\n",
      " [ -3.060538 ]\n",
      " [ -6.765616 ]\n",
      " [ -9.879764 ]\n",
      " [-10.092523 ]\n",
      " [ -9.347146 ]\n",
      " [ -5.25779  ]\n",
      " [-11.198097 ]\n",
      " [ -7.460388 ]\n",
      " [ -6.325221 ]\n",
      " [ -7.1763334]\n",
      " [-11.239203 ]\n",
      " [ -7.6282234]\n",
      " [ -9.481537 ]\n",
      " [-11.198097 ]\n",
      " [-11.030023 ]\n",
      " [ -5.2152476]\n",
      " [-12.005771 ]\n",
      " [-11.739386 ]\n",
      " [ -7.7458315]\n",
      " [ -3.3603806]\n",
      " [ -3.6546881]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: -1175.8359304015366\n",
      "Iteration 18\n",
      "Meta Reward: 398.7058413611345\n",
      "reward tf.Tensor(\n",
      "[[-3.2572336]\n",
      " [-3.6925225]\n",
      " [-3.5439901]\n",
      " [-3.6970963]\n",
      " [-3.8435516]\n",
      " [-3.8598123]\n",
      " [-4.343869 ]\n",
      " [-4.361848 ]\n",
      " [-4.4466715]\n",
      " [-3.5555882]\n",
      " [-3.5471783]\n",
      " [-4.7840104]\n",
      " [-3.1022875]\n",
      " [-3.643489 ]\n",
      " [-4.6004677]\n",
      " [-4.3177934]\n",
      " [-3.9221447]\n",
      " [-4.3341985]\n",
      " [-4.4149055]\n",
      " [-4.0879693]\n",
      " [-3.4126263]\n",
      " [-3.0051093]\n",
      " [-4.5093284]\n",
      " [-3.930304 ]\n",
      " [-3.669024 ]\n",
      " [-3.1166265]\n",
      " [-4.3177934]\n",
      " [-4.2296815]\n",
      " [-3.3505702]\n",
      " [-4.0840974]\n",
      " [-3.9221447]\n",
      " [-4.1882105]\n",
      " [-3.1022875]\n",
      " [-3.4533005]\n",
      " [-3.1683831]\n",
      " [-4.1975427]\n",
      " [-4.378136 ]\n",
      " [-4.831724 ]\n",
      " [-4.489741 ]\n",
      " [-4.5170693]\n",
      " [-4.4988904]\n",
      " [-4.2702284]\n",
      " [-3.983463 ]\n",
      " [-3.3035583]\n",
      " [-4.3341985]\n",
      " [-3.4809556]\n",
      " [-3.8650374]\n",
      " [-4.8486505]\n",
      " [-4.666879 ]\n",
      " [-3.0026433]\n",
      " [-3.4126263]\n",
      " [-4.4466715]\n",
      " [-4.5170693]\n",
      " [-4.0482244]\n",
      " [-4.443017 ]\n",
      " [-4.3554044]\n",
      " [-4.4810863]\n",
      " [-3.5799556]\n",
      " [-3.8870635]\n",
      " [-3.8598123]\n",
      " [-4.5170693]\n",
      " [-4.4607496]\n",
      " [-4.4429955]\n",
      " [-4.4149055]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1168.4089393577067\n",
      "Iteration 19\n",
      "Meta Reward: -372.16838641616914\n",
      "reward tf.Tensor(\n",
      "[[ -5.614008 ]\n",
      " [ -2.7357838]\n",
      " [-16.564386 ]\n",
      " [-14.494095 ]\n",
      " [-14.055548 ]\n",
      " [ -3.6516573]\n",
      " [-11.320373 ]\n",
      " [ -3.1570575]\n",
      " [-13.21972  ]\n",
      " [ -3.6516573]\n",
      " [ -5.614008 ]\n",
      " [-15.230035 ]\n",
      " [ -3.6999586]\n",
      " [-14.055548 ]\n",
      " [ -9.095493 ]\n",
      " [ -3.1779969]\n",
      " [ -3.3921804]\n",
      " [ -2.925888 ]\n",
      " [ -5.049675 ]\n",
      " [ -3.9031265]\n",
      " [-12.934085 ]\n",
      " [ -2.9802463]\n",
      " [ -3.6516573]\n",
      " [ -6.6973243]\n",
      " [ -5.7408953]\n",
      " [ -7.144363 ]\n",
      " [ -9.779564 ]\n",
      " [ -5.780703 ]\n",
      " [ -6.832742 ]\n",
      " [-12.123342 ]\n",
      " [ -4.2345967]\n",
      " [-13.21972  ]\n",
      " [-16.911009 ]\n",
      " [ -4.112741 ]\n",
      " [ -2.7357838]\n",
      " [-10.141993 ]\n",
      " [-12.934085 ]\n",
      " [-12.210226 ]\n",
      " [-11.320373 ]\n",
      " [ -3.6516573]\n",
      " [ -2.925888 ]\n",
      " [ -7.95152  ]\n",
      " [ -8.7643   ]\n",
      " [-16.942162 ]\n",
      " [ -7.144363 ]\n",
      " [ -4.5730605]\n",
      " [ -3.8120506]\n",
      " [ -5.780703 ]\n",
      " [-16.564386 ]\n",
      " [ -3.3359177]\n",
      " [-17.075968 ]\n",
      " [ -2.9186294]\n",
      " [ -3.9031265]\n",
      " [ -2.9359944]\n",
      " [ -3.3921804]\n",
      " [-11.479606 ]\n",
      " [ -3.3921804]\n",
      " [ -2.9802463]\n",
      " [-12.934085 ]\n",
      " [-10.92195  ]\n",
      " [ -4.544124 ]\n",
      " [-16.911009 ]\n",
      " [-15.985703 ]\n",
      " [ -3.6456416]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1190.6119568088368\n",
      "Iteration 20\n",
      "Meta Reward: 20.469921970039877\n",
      "reward tf.Tensor(\n",
      "[[-4.0434594]\n",
      " [-5.4895906]\n",
      " [-4.995819 ]\n",
      " [-5.3732367]\n",
      " [-5.62562  ]\n",
      " [-3.4154146]\n",
      " [-4.328406 ]\n",
      " [-4.8061886]\n",
      " [-4.4996758]\n",
      " [-5.461779 ]\n",
      " [-4.691595 ]\n",
      " [-3.8665457]\n",
      " [-4.465554 ]\n",
      " [-3.3847666]\n",
      " [-3.8665457]\n",
      " [-4.6947885]\n",
      " [-4.995819 ]\n",
      " [-5.7910347]\n",
      " [-6.202388 ]\n",
      " [-5.4895906]\n",
      " [-4.8061886]\n",
      " [-4.995819 ]\n",
      " [-5.1610193]\n",
      " [-4.995819 ]\n",
      " [-6.099292 ]\n",
      " [-3.9274745]\n",
      " [-4.995819 ]\n",
      " [-4.995819 ]\n",
      " [-6.2135954]\n",
      " [-4.924782 ]\n",
      " [-4.6947885]\n",
      " [-4.328406 ]\n",
      " [-6.0432854]\n",
      " [-5.377267 ]\n",
      " [-6.270154 ]\n",
      " [-4.355766 ]\n",
      " [-5.7910347]\n",
      " [-4.595263 ]\n",
      " [-6.0752344]\n",
      " [-4.25395  ]\n",
      " [-5.54023  ]\n",
      " [-3.733975 ]\n",
      " [-6.1507244]\n",
      " [-5.243007 ]\n",
      " [-5.4203615]\n",
      " [-5.797376 ]\n",
      " [-5.502777 ]\n",
      " [-4.995819 ]\n",
      " [-4.595263 ]\n",
      " [-6.106021 ]\n",
      " [-5.4314823]\n",
      " [-5.4314823]\n",
      " [-5.2235155]\n",
      " [-4.12899  ]\n",
      " [-3.869595 ]\n",
      " [-5.0244017]\n",
      " [-6.1507244]\n",
      " [-5.4203615]\n",
      " [-6.202388 ]\n",
      " [-4.25395  ]\n",
      " [-4.737097 ]\n",
      " [-5.502777 ]\n",
      " [-3.9986105]\n",
      " [-6.099292 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1192.2193750848521\n",
      "Iteration 21\n",
      "Meta Reward: -334.3739825161472\n",
      "reward tf.Tensor(\n",
      "[[-18.15591  ]\n",
      " [-17.006779 ]\n",
      " [ -6.239295 ]\n",
      " [-12.275052 ]\n",
      " [-14.685492 ]\n",
      " [ -9.060597 ]\n",
      " [-14.668071 ]\n",
      " [-17.804403 ]\n",
      " [ -4.229875 ]\n",
      " [ -7.4651046]\n",
      " [ -5.600863 ]\n",
      " [ -8.039547 ]\n",
      " [-16.807434 ]\n",
      " [ -3.4727786]\n",
      " [ -5.9410176]\n",
      " [-12.321511 ]\n",
      " [ -8.098544 ]\n",
      " [-14.349652 ]\n",
      " [ -7.168095 ]\n",
      " [-10.707552 ]\n",
      " [ -3.7974837]\n",
      " [ -6.246289 ]\n",
      " [-15.574145 ]\n",
      " [ -5.3566346]\n",
      " [ -3.1515367]\n",
      " [ -8.136886 ]\n",
      " [-13.500682 ]\n",
      " [ -6.239295 ]\n",
      " [-13.573577 ]\n",
      " [ -8.624105 ]\n",
      " [ -2.662001 ]\n",
      " [-16.914173 ]\n",
      " [-15.336747 ]\n",
      " [ -8.039547 ]\n",
      " [-13.64857  ]\n",
      " [ -3.0867302]\n",
      " [ -3.1595023]\n",
      " [-10.483391 ]\n",
      " [-15.186751 ]\n",
      " [-13.500682 ]\n",
      " [ -5.9410176]\n",
      " [-14.035298 ]\n",
      " [-15.186751 ]\n",
      " [ -6.246289 ]\n",
      " [ -6.246289 ]\n",
      " [ -3.6300032]\n",
      " [ -3.706291 ]\n",
      " [-10.707552 ]\n",
      " [-10.707552 ]\n",
      " [ -4.6120515]\n",
      " [-12.171741 ]\n",
      " [-10.006495 ]\n",
      " [-16.914173 ]\n",
      " [ -8.098544 ]\n",
      " [ -9.079205 ]\n",
      " [ -5.110335 ]\n",
      " [-12.280232 ]\n",
      " [ -3.1326296]\n",
      " [ -2.953935 ]\n",
      " [-16.24247  ]\n",
      " [-11.053697 ]\n",
      " [-10.63535  ]\n",
      " [-14.035298 ]\n",
      " [ -8.039547 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1201.1540640181565\n",
      "Iteration 22\n",
      "Meta Reward: 349.4559114385888\n",
      "reward tf.Tensor(\n",
      "[[-4.1511974]\n",
      " [-5.5092134]\n",
      " [-3.769066 ]\n",
      " [-6.6289616]\n",
      " [-6.094304 ]\n",
      " [-5.8071394]\n",
      " [-4.732584 ]\n",
      " [-5.107057 ]\n",
      " [-5.85824  ]\n",
      " [-3.4321508]\n",
      " [-5.831518 ]\n",
      " [-4.1046753]\n",
      " [-6.3094087]\n",
      " [-5.8793855]\n",
      " [-4.439689 ]\n",
      " [-5.373914 ]\n",
      " [-4.8570538]\n",
      " [-4.4233446]\n",
      " [-4.732584 ]\n",
      " [-3.7689824]\n",
      " [-3.7048857]\n",
      " [-4.1511974]\n",
      " [-4.1037025]\n",
      " [-5.599247 ]\n",
      " [-6.425476 ]\n",
      " [-5.8793855]\n",
      " [-5.65583  ]\n",
      " [-5.4294844]\n",
      " [-4.8047423]\n",
      " [-5.0310793]\n",
      " [-6.521942 ]\n",
      " [-4.1749153]\n",
      " [-4.499816 ]\n",
      " [-4.1037025]\n",
      " [-5.9460154]\n",
      " [-4.732584 ]\n",
      " [-4.1648083]\n",
      " [-5.0997972]\n",
      " [-4.35326  ]\n",
      " [-4.803507 ]\n",
      " [-3.769066 ]\n",
      " [-5.2981744]\n",
      " [-4.16182  ]\n",
      " [-4.16182  ]\n",
      " [-6.2858696]\n",
      " [-5.2096586]\n",
      " [-4.97198  ]\n",
      " [-5.9460154]\n",
      " [-5.910246 ]\n",
      " [-5.669093 ]\n",
      " [-5.827464 ]\n",
      " [-5.1498055]\n",
      " [-5.3518515]\n",
      " [-5.8721566]\n",
      " [-4.35326  ]\n",
      " [-5.9698186]\n",
      " [-6.6289616]\n",
      " [-5.512575 ]\n",
      " [-4.1046753]\n",
      " [-5.1264133]\n",
      " [-4.1724005]\n",
      " [-5.732182 ]\n",
      " [-3.4536726]\n",
      " [-4.1046753]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1208.8128882575893\n",
      "Iteration 23\n",
      "Meta Reward: 189.85519187722684\n",
      "reward tf.Tensor(\n",
      "[[ -7.8428016]\n",
      " [ -8.3379545]\n",
      " [ -7.0130286]\n",
      " [ -6.413266 ]\n",
      " [ -8.3379545]\n",
      " [ -5.6443567]\n",
      " [ -6.794505 ]\n",
      " [ -4.735749 ]\n",
      " [ -9.367986 ]\n",
      " [ -3.8167355]\n",
      " [ -8.104169 ]\n",
      " [ -3.5870388]\n",
      " [-10.997367 ]\n",
      " [ -3.732082 ]\n",
      " [ -3.3732712]\n",
      " [ -8.919133 ]\n",
      " [-10.64359  ]\n",
      " [-11.410055 ]\n",
      " [-12.38372  ]\n",
      " [ -9.033643 ]\n",
      " [-10.71457  ]\n",
      " [ -7.071663 ]\n",
      " [-10.706395 ]\n",
      " [-11.509991 ]\n",
      " [ -5.6476655]\n",
      " [ -4.3551455]\n",
      " [ -4.8437405]\n",
      " [ -9.374499 ]\n",
      " [ -4.018716 ]\n",
      " [-11.204198 ]\n",
      " [ -9.942247 ]\n",
      " [ -3.746595 ]\n",
      " [-11.204198 ]\n",
      " [ -4.0798473]\n",
      " [-10.71457  ]\n",
      " [ -8.190937 ]\n",
      " [ -5.6443567]\n",
      " [ -4.826053 ]\n",
      " [-12.157645 ]\n",
      " [ -5.6476655]\n",
      " [-11.114336 ]\n",
      " [ -3.3732712]\n",
      " [-12.157645 ]\n",
      " [ -4.571353 ]\n",
      " [ -9.510136 ]\n",
      " [ -8.765606 ]\n",
      " [ -9.374499 ]\n",
      " [ -4.735749 ]\n",
      " [-12.938842 ]\n",
      " [-13.120276 ]\n",
      " [ -3.8167355]\n",
      " [ -8.190937 ]\n",
      " [ -9.779728 ]\n",
      " [ -6.794505 ]\n",
      " [-10.997367 ]\n",
      " [-12.734881 ]\n",
      " [-12.873723 ]\n",
      " [-10.706395 ]\n",
      " [ -4.4808035]\n",
      " [ -4.573886 ]\n",
      " [ -4.571353 ]\n",
      " [ -4.4055214]\n",
      " [-13.120276 ]\n",
      " [ -4.702648 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: -1220.7584851955817\n",
      "Iteration 24\n",
      "Meta Reward: 189.9486666983853\n",
      "reward tf.Tensor(\n",
      "[[ -3.435881 ]\n",
      " [ -2.5782096]\n",
      " [-15.112237 ]\n",
      " [ -4.682444 ]\n",
      " [ -2.5782096]\n",
      " [ -8.048191 ]\n",
      " [ -4.1649776]\n",
      " [ -2.100511 ]\n",
      " [-13.527828 ]\n",
      " [ -5.371636 ]\n",
      " [ -9.389172 ]\n",
      " [ -3.5977335]\n",
      " [-10.101246 ]\n",
      " [ -4.183123 ]\n",
      " [-15.3289175]\n",
      " [-11.644677 ]\n",
      " [ -8.338265 ]\n",
      " [ -5.3053365]\n",
      " [ -8.54025  ]\n",
      " [ -2.5319016]\n",
      " [ -2.4429047]\n",
      " [ -9.614558 ]\n",
      " [-12.734513 ]\n",
      " [-13.474194 ]\n",
      " [-11.644677 ]\n",
      " [ -2.609691 ]\n",
      " [ -4.1161437]\n",
      " [ -5.670913 ]\n",
      " [ -6.909388 ]\n",
      " [ -8.221914 ]\n",
      " [ -3.7334325]\n",
      " [-17.12379  ]\n",
      " [ -4.682444 ]\n",
      " [-15.463755 ]\n",
      " [-12.487363 ]\n",
      " [ -2.100511 ]\n",
      " [ -3.435881 ]\n",
      " [-10.990942 ]\n",
      " [ -7.3089423]\n",
      " [ -5.371636 ]\n",
      " [ -2.2298157]\n",
      " [ -3.094442 ]\n",
      " [ -2.5538127]\n",
      " [-16.9489   ]\n",
      " [ -8.338265 ]\n",
      " [ -2.2298157]\n",
      " [ -3.792505 ]\n",
      " [-16.928675 ]\n",
      " [-16.9489   ]\n",
      " [-13.16729  ]\n",
      " [-14.892171 ]\n",
      " [-12.487363 ]\n",
      " [ -4.8881903]\n",
      " [-19.649183 ]\n",
      " [-17.790874 ]\n",
      " [ -4.682444 ]\n",
      " [ -9.614558 ]\n",
      " [-16.928675 ]\n",
      " [ -2.100511 ]\n",
      " [-18.691109 ]\n",
      " [-11.717608 ]\n",
      " [ -4.1161437]\n",
      " [ -5.3053365]\n",
      " [ -7.3727508]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1242.0613043482374\n",
      "Iteration 25\n",
      "Meta Reward: 580.648924373836\n",
      "reward tf.Tensor(\n",
      "[[-10.0186   ]\n",
      " [ -2.0665288]\n",
      " [ -5.042137 ]\n",
      " [-15.499346 ]\n",
      " [-10.61475  ]\n",
      " [ -7.8049583]\n",
      " [ -5.406376 ]\n",
      " [-11.714913 ]\n",
      " [-17.303566 ]\n",
      " [-18.70969  ]\n",
      " [ -7.503505 ]\n",
      " [-17.443985 ]\n",
      " [-11.926084 ]\n",
      " [ -3.5366464]\n",
      " [ -7.8049583]\n",
      " [ -7.148508 ]\n",
      " [ -7.8049583]\n",
      " [ -2.0665288]\n",
      " [ -2.8821309]\n",
      " [-13.878492 ]\n",
      " [ -2.5350318]\n",
      " [ -8.606431 ]\n",
      " [-17.443985 ]\n",
      " [ -1.9645941]\n",
      " [-12.270001 ]\n",
      " [ -2.1496277]\n",
      " [ -2.3879237]\n",
      " [ -2.1496277]\n",
      " [ -3.4418716]\n",
      " [ -3.7698722]\n",
      " [ -2.7350924]\n",
      " [-14.601137 ]\n",
      " [ -2.502028 ]\n",
      " [ -3.2204142]\n",
      " [ -3.0308743]\n",
      " [ -3.975196 ]\n",
      " [ -2.5350318]\n",
      " [ -2.851458 ]\n",
      " [-17.443985 ]\n",
      " [ -8.509192 ]\n",
      " [ -3.7698722]\n",
      " [ -3.4418716]\n",
      " [ -3.410585 ]\n",
      " [ -4.621435 ]\n",
      " [ -3.2204142]\n",
      " [ -6.001397 ]\n",
      " [-16.14082  ]\n",
      " [ -6.120467 ]\n",
      " [-10.61475  ]\n",
      " [-14.025395 ]\n",
      " [ -7.503505 ]\n",
      " [ -2.362438 ]\n",
      " [ -2.5350318]\n",
      " [ -7.2120223]\n",
      " [-18.70969  ]\n",
      " [ -3.8787568]\n",
      " [-10.279975 ]\n",
      " [ -6.305994 ]\n",
      " [ -3.7698722]\n",
      " [ -3.4418716]\n",
      " [-19.138136 ]\n",
      " [-16.891602 ]\n",
      " [ -3.4418716]\n",
      " [-18.531322 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1257.0915321257175\n",
      "Iteration 26\n",
      "Meta Reward: 434.56268771493615\n",
      "reward tf.Tensor(\n",
      "[[-3.904979 ]\n",
      " [-3.920838 ]\n",
      " [-3.899804 ]\n",
      " [-4.236916 ]\n",
      " [-4.0834413]\n",
      " [-3.8356102]\n",
      " [-3.9180784]\n",
      " [-4.247272 ]\n",
      " [-4.0964427]\n",
      " [-3.915675 ]\n",
      " [-4.0014453]\n",
      " [-3.9053771]\n",
      " [-3.8922446]\n",
      " [-3.784678 ]\n",
      " [-3.7234547]\n",
      " [-4.0643454]\n",
      " [-4.1171317]\n",
      " [-4.247272 ]\n",
      " [-3.6375396]\n",
      " [-3.8046765]\n",
      " [-4.177933 ]\n",
      " [-4.0046024]\n",
      " [-3.7843106]\n",
      " [-4.034592 ]\n",
      " [-3.93896  ]\n",
      " [-4.0976086]\n",
      " [-3.904979 ]\n",
      " [-4.1397023]\n",
      " [-4.123859 ]\n",
      " [-3.7545342]\n",
      " [-4.1085997]\n",
      " [-3.6675773]\n",
      " [-3.6442935]\n",
      " [-4.13623  ]\n",
      " [-4.0014453]\n",
      " [-3.9216871]\n",
      " [-3.9180784]\n",
      " [-3.9444046]\n",
      " [-4.2027435]\n",
      " [-3.6631775]\n",
      " [-3.9116955]\n",
      " [-4.2509036]\n",
      " [-3.6481776]\n",
      " [-3.9170012]\n",
      " [-4.13623  ]\n",
      " [-4.0948706]\n",
      " [-4.0948706]\n",
      " [-3.9180784]\n",
      " [-4.2027435]\n",
      " [-3.93896  ]\n",
      " [-4.2221937]\n",
      " [-4.2397113]\n",
      " [-3.8607464]\n",
      " [-3.9175735]\n",
      " [-4.006539 ]\n",
      " [-3.8126903]\n",
      " [-4.2356186]\n",
      " [-3.8895097]\n",
      " [-3.8569846]\n",
      " [-4.123859 ]\n",
      " [-4.177933 ]\n",
      " [-4.0590177]\n",
      " [-4.1934724]\n",
      " [-3.6589723]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1275.9021969083199\n",
      "Iteration 27\n",
      "Meta Reward: 697.8534570172976\n",
      "reward tf.Tensor(\n",
      "[[ -8.624456 ]\n",
      " [ -2.5900342]\n",
      " [ -5.378684 ]\n",
      " [ -6.2120905]\n",
      " [-14.633208 ]\n",
      " [ -2.1052415]\n",
      " [-17.063383 ]\n",
      " [ -2.4373875]\n",
      " [ -3.5558615]\n",
      " [ -8.498524 ]\n",
      " [ -6.2120905]\n",
      " [-10.154778 ]\n",
      " [-17.12202  ]\n",
      " [ -2.8996873]\n",
      " [-14.633208 ]\n",
      " [ -4.947137 ]\n",
      " [ -4.735297 ]\n",
      " [ -3.8954906]\n",
      " [ -4.410717 ]\n",
      " [-11.542717 ]\n",
      " [-13.251922 ]\n",
      " [-16.139893 ]\n",
      " [-10.172522 ]\n",
      " [-13.251922 ]\n",
      " [-14.8172035]\n",
      " [ -3.109922 ]\n",
      " [ -3.4841127]\n",
      " [-10.172522 ]\n",
      " [ -4.5132713]\n",
      " [ -6.5363455]\n",
      " [-17.063383 ]\n",
      " [ -4.1692915]\n",
      " [-11.20466  ]\n",
      " [-13.251922 ]\n",
      " [ -8.498524 ]\n",
      " [ -3.5558615]\n",
      " [ -3.5909276]\n",
      " [-14.8172035]\n",
      " [-16.855803 ]\n",
      " [ -5.7786083]\n",
      " [ -7.277845 ]\n",
      " [-16.994665 ]\n",
      " [-11.693415 ]\n",
      " [ -3.109922 ]\n",
      " [ -5.378684 ]\n",
      " [ -9.695749 ]\n",
      " [ -7.560228 ]\n",
      " [ -3.9171157]\n",
      " [ -4.5132713]\n",
      " [ -4.513832 ]\n",
      " [-13.299919 ]\n",
      " [-12.907636 ]\n",
      " [-10.172522 ]\n",
      " [-12.907636 ]\n",
      " [ -2.8107615]\n",
      " [ -8.29907  ]\n",
      " [ -2.5900342]\n",
      " [-11.693415 ]\n",
      " [ -3.7430317]\n",
      " [-13.251922 ]\n",
      " [ -3.8954906]\n",
      " [ -4.6680737]\n",
      " [ -3.6635041]\n",
      " [-11.693415 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1290.8230784095922\n",
      "Iteration 28\n",
      "Meta Reward: 712.237480720046\n",
      "reward tf.Tensor(\n",
      "[[-10.969269 ]\n",
      " [-11.939001 ]\n",
      " [ -2.820364 ]\n",
      " [-11.669994 ]\n",
      " [ -2.8230255]\n",
      " [ -2.7738113]\n",
      " [ -2.8230255]\n",
      " [-11.265932 ]\n",
      " [-11.104776 ]\n",
      " [-11.403258 ]\n",
      " [-12.055258 ]\n",
      " [ -9.159168 ]\n",
      " [-10.559208 ]\n",
      " [ -4.1291213]\n",
      " [-11.888664 ]\n",
      " [ -4.1291213]\n",
      " [ -7.954987 ]\n",
      " [ -3.9060032]\n",
      " [ -6.296427 ]\n",
      " [ -9.9363785]\n",
      " [ -5.235248 ]\n",
      " [ -4.933309 ]\n",
      " [-12.004721 ]\n",
      " [-12.004721 ]\n",
      " [-11.97334  ]\n",
      " [ -4.1656756]\n",
      " [ -2.9262295]\n",
      " [-11.375667 ]\n",
      " [-11.265932 ]\n",
      " [-12.221516 ]\n",
      " [ -6.59473  ]\n",
      " [-12.378457 ]\n",
      " [ -7.954987 ]\n",
      " [-11.265932 ]\n",
      " [-12.050051 ]\n",
      " [ -6.6360307]\n",
      " [ -5.9040403]\n",
      " [ -4.5611997]\n",
      " [-10.559208 ]\n",
      " [ -5.3361   ]\n",
      " [-12.06675  ]\n",
      " [ -6.296427 ]\n",
      " [-12.050051 ]\n",
      " [-11.843592 ]\n",
      " [-12.26351  ]\n",
      " [ -7.9760723]\n",
      " [ -8.931887 ]\n",
      " [-10.215246 ]\n",
      " [-11.669994 ]\n",
      " [ -7.954987 ]\n",
      " [ -4.1291213]\n",
      " [ -9.970607 ]\n",
      " [ -4.039359 ]\n",
      " [ -7.9760723]\n",
      " [ -4.039359 ]\n",
      " [ -7.532785 ]\n",
      " [-10.276312 ]\n",
      " [-12.378457 ]\n",
      " [-11.843592 ]\n",
      " [ -2.6699457]\n",
      " [ -5.7070813]\n",
      " [-10.215246 ]\n",
      " [ -8.931887 ]\n",
      " [ -6.1044717]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1286.456673301918\n",
      "Iteration 29\n",
      "Meta Reward: -0.8417309154774557\n",
      "reward tf.Tensor(\n",
      "[[-3.9168968]\n",
      " [-6.1315613]\n",
      " [-6.7796144]\n",
      " [-4.683801 ]\n",
      " [-7.064626 ]\n",
      " [-3.6994321]\n",
      " [-6.727087 ]\n",
      " [-6.875978 ]\n",
      " [-4.8408113]\n",
      " [-6.6460023]\n",
      " [-6.3853583]\n",
      " [-5.8317513]\n",
      " [-6.7796144]\n",
      " [-4.0966277]\n",
      " [-5.12469  ]\n",
      " [-6.3989463]\n",
      " [-3.8010826]\n",
      " [-4.28858  ]\n",
      " [-5.737432 ]\n",
      " [-5.996521 ]\n",
      " [-4.28858  ]\n",
      " [-5.9807234]\n",
      " [-6.7892118]\n",
      " [-5.737432 ]\n",
      " [-7.12303  ]\n",
      " [-6.875978 ]\n",
      " [-6.875978 ]\n",
      " [-5.470383 ]\n",
      " [-6.645923 ]\n",
      " [-6.981838 ]\n",
      " [-5.8503604]\n",
      " [-5.733999 ]\n",
      " [-5.853575 ]\n",
      " [-6.1769996]\n",
      " [-6.601329 ]\n",
      " [-6.7892118]\n",
      " [-5.5683517]\n",
      " [-3.905331 ]\n",
      " [-6.6346183]\n",
      " [-6.3989463]\n",
      " [-6.6346183]\n",
      " [-6.1315613]\n",
      " [-5.684952 ]\n",
      " [-3.905331 ]\n",
      " [-6.2245116]\n",
      " [-3.0333295]\n",
      " [-5.4322395]\n",
      " [-7.154632 ]\n",
      " [-5.9807234]\n",
      " [-6.7892118]\n",
      " [-5.174193 ]\n",
      " [-3.8010826]\n",
      " [-3.9178982]\n",
      " [-5.2577605]\n",
      " [-4.840383 ]\n",
      " [-3.8115532]\n",
      " [-6.583583 ]\n",
      " [-5.470383 ]\n",
      " [-2.9381526]\n",
      " [-3.7830615]\n",
      " [-3.5086577]\n",
      " [-3.9528613]\n",
      " [-5.853575 ]\n",
      " [-6.1769996]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: -1299.4726956225952\n",
      "Iteration 30\n",
      "Meta Reward: 320.7599500783831\n",
      "reward tf.Tensor(\n",
      "[[-10.66224  ]\n",
      " [-12.972089 ]\n",
      " [ -5.166164 ]\n",
      " [ -2.1306105]\n",
      " [ -1.979524 ]\n",
      " [-14.599183 ]\n",
      " [ -4.974274 ]\n",
      " [ -4.4662223]\n",
      " [ -9.355294 ]\n",
      " [ -6.558875 ]\n",
      " [ -1.6719166]\n",
      " [ -4.0044694]\n",
      " [ -1.5173577]\n",
      " [ -1.979524 ]\n",
      " [-14.599183 ]\n",
      " [-14.96679  ]\n",
      " [ -3.1623938]\n",
      " [ -5.894989 ]\n",
      " [ -5.7623525]\n",
      " [-14.98239  ]\n",
      " [ -2.9269826]\n",
      " [ -8.106616 ]\n",
      " [ -4.042977 ]\n",
      " [ -3.2860765]\n",
      " [ -3.2860765]\n",
      " [ -2.7791967]\n",
      " [ -7.131516 ]\n",
      " [ -1.6600894]\n",
      " [ -8.106616 ]\n",
      " [ -9.875187 ]\n",
      " [ -2.0936852]\n",
      " [ -1.9929808]\n",
      " [ -3.500944 ]\n",
      " [ -8.408305 ]\n",
      " [ -2.8887765]\n",
      " [-13.666183 ]\n",
      " [ -3.541859 ]\n",
      " [-12.950157 ]\n",
      " [-13.909109 ]\n",
      " [ -9.875187 ]\n",
      " [-14.98239  ]\n",
      " [-13.666183 ]\n",
      " [-10.66224  ]\n",
      " [ -2.826461 ]\n",
      " [-11.717641 ]\n",
      " [ -2.2084355]\n",
      " [ -3.4471586]\n",
      " [ -3.541859 ]\n",
      " [-13.617732 ]\n",
      " [ -6.9449725]\n",
      " [ -2.8539488]\n",
      " [ -2.826461 ]\n",
      " [ -3.8589432]\n",
      " [ -1.6600894]\n",
      " [ -2.9269826]\n",
      " [ -9.196153 ]\n",
      " [ -1.6600894]\n",
      " [-14.199085 ]\n",
      " [ -7.131516 ]\n",
      " [ -3.4248521]\n",
      " [ -1.5173577]\n",
      " [ -9.875187 ]\n",
      " [-12.950157 ]\n",
      " [ -2.9417255]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1290.6752559029974\n",
      "Iteration 31\n",
      "Meta Reward: -249.0499416851144\n",
      "reward tf.Tensor(\n",
      "[[ -3.070012 ]\n",
      " [ -7.7723436]\n",
      " [ -4.129321 ]\n",
      " [ -2.55209  ]\n",
      " [ -8.356219 ]\n",
      " [-10.894218 ]\n",
      " [ -2.3248365]\n",
      " [ -4.2874107]\n",
      " [ -3.947004 ]\n",
      " [ -3.5446937]\n",
      " [ -2.4699144]\n",
      " [ -3.947004 ]\n",
      " [ -4.948103 ]\n",
      " [-10.33551  ]\n",
      " [ -2.4699144]\n",
      " [ -6.5122237]\n",
      " [ -8.4534645]\n",
      " [ -9.5235815]\n",
      " [ -9.977051 ]\n",
      " [ -6.608021 ]\n",
      " [ -4.58535  ]\n",
      " [ -8.4534645]\n",
      " [-10.954378 ]\n",
      " [-10.014779 ]\n",
      " [ -8.4534645]\n",
      " [ -7.230241 ]\n",
      " [ -8.636608 ]\n",
      " [ -2.406565 ]\n",
      " [ -8.969642 ]\n",
      " [ -4.332926 ]\n",
      " [ -9.147039 ]\n",
      " [ -5.436786 ]\n",
      " [ -2.7305493]\n",
      " [ -8.475821 ]\n",
      " [ -2.2957668]\n",
      " [-10.695503 ]\n",
      " [-10.894218 ]\n",
      " [ -4.6672096]\n",
      " [ -5.2147675]\n",
      " [ -7.43157  ]\n",
      " [ -8.441547 ]\n",
      " [ -4.129321 ]\n",
      " [ -9.147039 ]\n",
      " [ -8.636608 ]\n",
      " [ -5.0145693]\n",
      " [ -6.608021 ]\n",
      " [ -2.846112 ]\n",
      " [ -2.3248365]\n",
      " [ -3.3486826]\n",
      " [ -9.773991 ]\n",
      " [ -6.5977125]\n",
      " [ -6.5122237]\n",
      " [ -8.475821 ]\n",
      " [ -2.3248365]\n",
      " [-10.894218 ]\n",
      " [ -3.070012 ]\n",
      " [ -9.710822 ]\n",
      " [ -7.9382915]\n",
      " [ -2.5445325]\n",
      " [ -2.6914363]\n",
      " [-10.808352 ]\n",
      " [-10.42816  ]\n",
      " [ -4.129321 ]\n",
      " [ -2.55209  ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1306.001821844922\n",
      "Iteration 32\n",
      "Meta Reward: 597.9932172329277\n",
      "reward tf.Tensor(\n",
      "[[-10.1455145]\n",
      " [ -9.9977045]\n",
      " [ -2.9262228]\n",
      " [ -4.8329654]\n",
      " [ -2.0653834]\n",
      " [-10.645754 ]\n",
      " [ -8.008496 ]\n",
      " [-10.1455145]\n",
      " [ -8.329032 ]\n",
      " [ -3.8533163]\n",
      " [ -9.028163 ]\n",
      " [ -9.9977045]\n",
      " [-10.855237 ]\n",
      " [ -1.9588569]\n",
      " [ -9.9977045]\n",
      " [ -1.7298158]\n",
      " [-10.52077  ]\n",
      " [ -3.9241197]\n",
      " [-10.594817 ]\n",
      " [ -2.3992019]\n",
      " [ -2.823509 ]\n",
      " [ -3.9241197]\n",
      " [ -5.31717  ]\n",
      " [-10.668088 ]\n",
      " [-10.52077  ]\n",
      " [ -4.3926396]\n",
      " [-12.176564 ]\n",
      " [ -1.7298158]\n",
      " [ -2.823509 ]\n",
      " [ -9.028163 ]\n",
      " [ -7.213472 ]\n",
      " [-10.691381 ]\n",
      " [-10.691381 ]\n",
      " [ -8.14229  ]\n",
      " [ -7.157209 ]\n",
      " [ -2.0653834]\n",
      " [ -2.7206333]\n",
      " [ -2.823509 ]\n",
      " [ -7.0006266]\n",
      " [ -5.9276824]\n",
      " [ -2.4032726]\n",
      " [ -2.788868 ]\n",
      " [ -7.21283  ]\n",
      " [-12.404934 ]\n",
      " [-11.271147 ]\n",
      " [ -2.0897253]\n",
      " [ -9.180564 ]\n",
      " [ -4.820913 ]\n",
      " [ -9.274877 ]\n",
      " [ -3.7770119]\n",
      " [ -8.14229  ]\n",
      " [ -7.0006266]\n",
      " [ -5.0265064]\n",
      " [ -3.7417898]\n",
      " [-11.471868 ]\n",
      " [-10.824343 ]\n",
      " [-11.019017 ]\n",
      " [ -2.0897253]\n",
      " [-10.668088 ]\n",
      " [ -5.0265064]\n",
      " [ -2.0653834]\n",
      " [-10.668088 ]\n",
      " [ -2.1419673]\n",
      " [ -3.8533163]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1305.3989814150273\n",
      "Iteration 33\n"
     ]
    }
   ],
   "source": [
    "# Step 4:\n",
    "for it in range(total_iterations):\n",
    "    print('Iteration', it)\n",
    "    # Step 5: Generate d0\n",
    "    data, _ = generate(teacher_actor, d0_rollout)\n",
    "    d0.append(data)\n",
    "    \n",
    "    # Step 6: Update pi to temp\n",
    "    # ********* ddpg_update的optimizer用 actor_optimizer (不知道會不會出事)\n",
    "    temp = copy(actor)\n",
    "    ddpg_update(d0, temp, actor_optimizer, critic, critic_optimizer, update_critic=False)\n",
    "    \n",
    "    # Step 7: \n",
    "    data, meta_pip = generate(temp, d1_rollout)\n",
    "    d1.append(data)\n",
    "    meta = meta_pip - meta_pi\n",
    "    print('Meta Reward:', meta)\n",
    "    \n",
    "    # Step 8:\n",
    "    loss = teacher_update(teacher_actor, teacher_actor_optim, teacher_critic, teacher_critic_optim, d0, meta)\n",
    "    print('Teacher Loss:', loss)\n",
    "    teacher_loss_list.append(loss)\n",
    "\n",
    "    # Step 9:\n",
    "    buffer.append(d0.raw)\n",
    "    buffer.append(d1.raw)\n",
    "    \n",
    "    # Step 10:\n",
    "    ddpg_update(buffer, actor, actor_optimizer, critic, critic_optimizer)\n",
    "    data_temp, meta_pi = generate(actor, d1_rollout)\n",
    "    \n",
    "    meta_reward_list.append(meta)\n",
    "    it_reward_list.append(meta_pi)\n",
    "    avg_reward = np.mean(it_reward_list[-40:])\n",
    "    print(\"Avg Reward:\", avg_reward)\n",
    "    avg_reward_list.append(avg_reward)\n",
    "    \n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Avg. Iteration Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(meta_reward_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Meta Iteration Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(teacher_loss_list)\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Teacher Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
