{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.capacity = capacity\n",
    "        self.counter = 0\n",
    "        self.batch_size = 64\n",
    "        self.state_buffer = np.zeros((self.capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.capacity, num_states))\n",
    "\n",
    "    def insert(self, obs_tuple):\n",
    "        index = self.index\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.counter += 1\n",
    "    \n",
    "    def append(self, d):\n",
    "        for i in d:\n",
    "            self.insert(i)\n",
    "    \n",
    "    def sample(self):\n",
    "        idx = np.random.choice(self.size, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[idx])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[idx])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[idx])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[idx])\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch\n",
    "    \n",
    "    @property\n",
    "    def raw(self):\n",
    "        d = [(self.state_buffer[i], self.action_buffer[i], self.reward_buffer[i], self.next_state_buffer[i]) for i in range(self.size)]\n",
    "        return d\n",
    "            \n",
    "    @property\n",
    "    def size(self):\n",
    "        if self.counter >= self.capacity:\n",
    "            return self.capacity\n",
    "        else:\n",
    "            return self.counter\n",
    "    \n",
    "    @property\n",
    "    def index(self):\n",
    "        return self.counter % self.capacity\n",
    "    \n",
    "    @property\n",
    "    def batches(self):\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer)\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer)\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer)\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ddpg_update(d, actor, actor_optimizer, critic, critic_optimizer, update_critic=True):\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = d.sample()\n",
    "    \n",
    "    if update_critic:\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = critic([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actions = actor(state_batch, training=True)\n",
    "        critic_value = critic([state_batch, actions], training=True)\n",
    "        # Used `-value` as we want to maximize the value given\n",
    "        # by the critic for our actions\n",
    "        actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "    actor_grad = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def target_update(target, model):\n",
    "    target.variables = model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def teacher_update(actor, actor_optim, critic, critic_optim, d, meta):\n",
    "    state_batch, action_batch, reward_batch, _ = d.sample()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        reward = critic([state_batch, action_batch], training=True)\n",
    "        critic_loss = tf.math.reduce_mean(tf.math.square(reward - reward_batch))\n",
    "    \n",
    "    critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "    critic_optim.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        target_actions = actor(state_batch, training=True)\n",
    "        reward = critic([state_batch, target_actions], training=False)\n",
    "        print('reward', reward)\n",
    "        log_reward = tf.math.log(reward + 0.00001)\n",
    "        print('log_reward', log_reward)\n",
    "        actor_loss = tf.math.multiply(tf.cast(meta, dtype=\"float32\"), tf.reduce_sum(log_reward))\n",
    "        \n",
    "    actor_grad = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optim.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "    \n",
    "    return actor_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give: policy(model) and current state\n",
    "# return: next action\n",
    "def next_action(policy, state):\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    sampled_actions = tf.squeeze(policy(state))\n",
    "    sampled_actions = sampled_actions.numpy()\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n",
    "# sample n steps using policy\n",
    "# give: policy, n\n",
    "# return: [sars], accumulated reward\n",
    "def generate(policy, n):\n",
    "    prev_state = env.reset()\n",
    "    d = []\n",
    "    total_reward = 0\n",
    "    for i in range(n):\n",
    "        action = next_action(policy, prev_state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        d.append((prev_state, action, reward, state))\n",
    "        prev_state = state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return d, total_reward\n",
    "\n",
    "def copy(model):\n",
    "    new_model = tf.keras.models.clone_model(model)\n",
    "    new_model.set_weights(model.get_weights())\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "teacher_lr = 0.0001\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "\n",
    "total_iterations = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "d0_rollout = 100\n",
    "d1_rollout = 200\n",
    "pi_rollout = 50\n",
    "buffer_capacity = 50000\n",
    "log_interval = 50\n",
    "\n",
    "# To store reward history of each episode\n",
    "it_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "meta_reward_list = []\n",
    "teacher_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize pi\n",
    "teacher_actor = get_actor()\n",
    "teacher_critic = get_critic()\n",
    "\n",
    "# Step 1: Initialize pie\n",
    "actor = get_actor()\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "critic = get_critic()\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "# Making the weights equal initially\n",
    "teacher_actor.set_weights(actor.get_weights())\n",
    "teacher_actor_optim = tf.keras.optimizers.Adam(actor_lr)\n",
    "teacher_critic.set_weights(critic.get_weights())\n",
    "teacher_critic_optim = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "# Step 2:\n",
    "d0, d1 = Buffer(d0_rollout), Buffer(d1_rollout)\n",
    "data, meta_pi = generate(actor, d1_rollout)\n",
    "d1.append(data)\n",
    "\n",
    "# Step 3:\n",
    "buffer = Buffer(buffer_capacity)\n",
    "buffer.append(d1.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Meta Reward: 460.6305112851918\n",
      "reward tf.Tensor(\n",
      "[[-0.07835543]\n",
      " [-0.061279  ]\n",
      " [-0.8340434 ]\n",
      " [-0.1008544 ]\n",
      " [-0.04810326]\n",
      " [-0.07242012]\n",
      " [-0.97741425]\n",
      " [-0.93841726]\n",
      " [-0.24525468]\n",
      " [-0.9975993 ]\n",
      " [-0.26687747]\n",
      " [-0.15630959]\n",
      " [-0.21214534]\n",
      " [-0.07173602]\n",
      " [-0.15630959]\n",
      " [-0.20921685]\n",
      " [-0.5729431 ]\n",
      " [-0.92038316]\n",
      " [-1.0195044 ]\n",
      " [-0.16722278]\n",
      " [-0.97741425]\n",
      " [-0.12742653]\n",
      " [-0.72332776]\n",
      " [-0.769971  ]\n",
      " [-0.9975993 ]\n",
      " [-0.92038316]\n",
      " [-0.72332776]\n",
      " [-0.19556242]\n",
      " [-0.111775  ]\n",
      " [-0.30444282]\n",
      " [-0.21214534]\n",
      " [-0.8340434 ]\n",
      " [-0.29177332]\n",
      " [-0.25399753]\n",
      " [-0.09998129]\n",
      " [-0.17692442]\n",
      " [-0.2677326 ]\n",
      " [-0.3045719 ]\n",
      " [-0.3045719 ]\n",
      " [-0.9975993 ]\n",
      " [-0.477905  ]\n",
      " [-0.12742653]\n",
      " [-0.8554919 ]\n",
      " [-0.92038316]\n",
      " [-0.93841726]\n",
      " [-0.81954646]\n",
      " [-0.81954646]\n",
      " [-0.21019422]\n",
      " [-1.0195044 ]\n",
      " [-0.13402422]\n",
      " [-0.81954646]\n",
      " [-0.3045719 ]\n",
      " [-0.76137936]\n",
      " [-0.14417212]\n",
      " [-0.30484262]\n",
      " [-0.8340434 ]\n",
      " [-0.30484262]\n",
      " [-0.21019422]\n",
      " [-0.9900609 ]\n",
      " [-0.8554919 ]\n",
      " [-0.15368778]\n",
      " [-0.8554919 ]\n",
      " [-0.28168806]\n",
      " [-0.12690865]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -718.3489854528323\n",
      "Iteration 1\n",
      "Meta Reward: -350.78632892236806\n",
      "reward tf.Tensor(\n",
      "[[-0.17514129]\n",
      " [-0.46399078]\n",
      " [-0.8588404 ]\n",
      " [-0.7038549 ]\n",
      " [-0.7760162 ]\n",
      " [-0.5032533 ]\n",
      " [-0.8607937 ]\n",
      " [-0.19578089]\n",
      " [-0.16892289]\n",
      " [-0.50974303]\n",
      " [-0.2795523 ]\n",
      " [-0.25069973]\n",
      " [-0.13669229]\n",
      " [-0.30233467]\n",
      " [-0.44527128]\n",
      " [-0.15897764]\n",
      " [-0.60125214]\n",
      " [-0.71107846]\n",
      " [-0.15674545]\n",
      " [-0.1372665 ]\n",
      " [-0.6322131 ]\n",
      " [-0.15207617]\n",
      " [-0.50974303]\n",
      " [-0.15674545]\n",
      " [-0.1457278 ]\n",
      " [-0.48630613]\n",
      " [-0.15207617]\n",
      " [-0.8345904 ]\n",
      " [-0.64561355]\n",
      " [-0.2199609 ]\n",
      " [-0.655958  ]\n",
      " [-0.2795523 ]\n",
      " [-0.22094364]\n",
      " [-0.14150839]\n",
      " [-0.17633696]\n",
      " [-0.15968373]\n",
      " [-0.13457534]\n",
      " [-0.35039878]\n",
      " [-0.28810558]\n",
      " [-0.73323995]\n",
      " [-0.44036105]\n",
      " [-0.32215405]\n",
      " [-0.13227706]\n",
      " [-0.5301469 ]\n",
      " [-0.28772268]\n",
      " [-0.17615044]\n",
      " [-0.28772268]\n",
      " [-0.24427761]\n",
      " [-0.1396156 ]\n",
      " [-0.3683199 ]\n",
      " [-0.17633696]\n",
      " [-0.16892289]\n",
      " [-0.20434552]\n",
      " [-0.85303205]\n",
      " [-0.19491966]\n",
      " [-0.17633696]\n",
      " [-0.42824003]\n",
      " [-0.17633696]\n",
      " [-0.25069973]\n",
      " [-0.8418909 ]\n",
      " [-0.2629108 ]\n",
      " [-0.21677686]\n",
      " [-0.23115972]\n",
      " [-0.15674545]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -873.4240773688721\n",
      "Iteration 2\n",
      "Meta Reward: -314.36857380771676\n",
      "reward tf.Tensor(\n",
      "[[-2.234188  ]\n",
      " [-1.0419773 ]\n",
      " [-0.21313412]\n",
      " [-1.8711159 ]\n",
      " [-0.21599439]\n",
      " [-1.1618332 ]\n",
      " [-1.798857  ]\n",
      " [-0.2029875 ]\n",
      " [-1.0419773 ]\n",
      " [-2.1018515 ]\n",
      " [-0.22236563]\n",
      " [-0.19804166]\n",
      " [-1.8711159 ]\n",
      " [-1.0419773 ]\n",
      " [-0.8150166 ]\n",
      " [-0.24347866]\n",
      " [-0.36790866]\n",
      " [-0.26851243]\n",
      " [-0.33970183]\n",
      " [-0.28587323]\n",
      " [-0.20805272]\n",
      " [-1.5745533 ]\n",
      " [-2.4471455 ]\n",
      " [-0.19804166]\n",
      " [-0.9191687 ]\n",
      " [-0.24484663]\n",
      " [-0.19642021]\n",
      " [-0.3200748 ]\n",
      " [-0.292663  ]\n",
      " [-0.30982512]\n",
      " [-0.2985794 ]\n",
      " [-1.1618332 ]\n",
      " [-0.33406258]\n",
      " [-0.2356811 ]\n",
      " [-2.1018515 ]\n",
      " [-0.34078157]\n",
      " [-0.2029875 ]\n",
      " [-0.9071463 ]\n",
      " [-0.8150166 ]\n",
      " [-0.31542918]\n",
      " [-0.4504511 ]\n",
      " [-0.54361737]\n",
      " [-0.20805272]\n",
      " [-0.44871885]\n",
      " [-0.3200748 ]\n",
      " [-0.3200748 ]\n",
      " [-0.3487113 ]\n",
      " [-0.34078157]\n",
      " [-0.30982512]\n",
      " [-0.8503434 ]\n",
      " [-0.2985794 ]\n",
      " [-0.32765114]\n",
      " [-0.30982512]\n",
      " [-0.28587323]\n",
      " [-1.5745533 ]\n",
      " [-2.234188  ]\n",
      " [-0.48217195]\n",
      " [-0.38663185]\n",
      " [-0.8077266 ]\n",
      " [-0.8150166 ]\n",
      " [-0.8077266 ]\n",
      " [-0.28656188]\n",
      " [-0.20028652]\n",
      " [-0.26545084]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -802.6939650772529\n",
      "Iteration 3\n",
      "Meta Reward: -772.6494761860621\n",
      "reward tf.Tensor(\n",
      "[[-0.63795215]\n",
      " [-0.4823394 ]\n",
      " [-0.36638057]\n",
      " [-0.38227364]\n",
      " [-0.7201866 ]\n",
      " [-0.3340792 ]\n",
      " [-0.34094504]\n",
      " [-1.1286197 ]\n",
      " [-0.2718488 ]\n",
      " [-1.1627315 ]\n",
      " [-0.59034544]\n",
      " [-0.8538324 ]\n",
      " [-0.5711874 ]\n",
      " [-0.6901884 ]\n",
      " [-0.5442877 ]\n",
      " [-0.278156  ]\n",
      " [-0.7822157 ]\n",
      " [-1.0860816 ]\n",
      " [-1.1657552 ]\n",
      " [-0.63991165]\n",
      " [-0.35703772]\n",
      " [-0.6883485 ]\n",
      " [-0.98649937]\n",
      " [-1.1803117 ]\n",
      " [-0.503113  ]\n",
      " [-0.7024791 ]\n",
      " [-0.31215772]\n",
      " [-0.57075137]\n",
      " [-0.57075137]\n",
      " [-0.7238967 ]\n",
      " [-0.6671616 ]\n",
      " [-0.3500319 ]\n",
      " [-0.503113  ]\n",
      " [-0.45428392]\n",
      " [-0.76331836]\n",
      " [-1.1427906 ]\n",
      " [-0.35703772]\n",
      " [-0.47092578]\n",
      " [-1.1819727 ]\n",
      " [-0.32988855]\n",
      " [-0.5496077 ]\n",
      " [-0.3340792 ]\n",
      " [-0.32988855]\n",
      " [-0.26857996]\n",
      " [-1.1920742 ]\n",
      " [-0.4823394 ]\n",
      " [-0.47092578]\n",
      " [-0.9239419 ]\n",
      " [-0.6883485 ]\n",
      " [-0.5711874 ]\n",
      " [-0.72918165]\n",
      " [-0.63991165]\n",
      " [-0.30109733]\n",
      " [-1.0949085 ]\n",
      " [-0.98662066]\n",
      " [-0.63991165]\n",
      " [-0.38417324]\n",
      " [-0.8472559 ]\n",
      " [-0.45428392]\n",
      " [-1.0383005 ]\n",
      " [-0.6883485 ]\n",
      " [-0.5711874 ]\n",
      " [-0.65794003]\n",
      " [-1.0383005 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -904.7409047403974\n",
      "Iteration 4\n",
      "Meta Reward: 253.3730811705617\n",
      "reward tf.Tensor(\n",
      "[[-0.44950923]\n",
      " [-2.3488517 ]\n",
      " [-1.174982  ]\n",
      " [-1.2596596 ]\n",
      " [-1.0218576 ]\n",
      " [-1.8206718 ]\n",
      " [-0.41905442]\n",
      " [-0.5891244 ]\n",
      " [-3.3795037 ]\n",
      " [-2.0224173 ]\n",
      " [-1.8122934 ]\n",
      " [-0.5946215 ]\n",
      " [-1.3952849 ]\n",
      " [-3.187001  ]\n",
      " [-3.1947308 ]\n",
      " [-1.0928438 ]\n",
      " [-2.5717146 ]\n",
      " [-1.0928438 ]\n",
      " [-2.713493  ]\n",
      " [-3.1947308 ]\n",
      " [-3.1947308 ]\n",
      " [-1.0928438 ]\n",
      " [-0.2660063 ]\n",
      " [-0.7274354 ]\n",
      " [-3.4361036 ]\n",
      " [-2.0224173 ]\n",
      " [-0.6341317 ]\n",
      " [-2.5582051 ]\n",
      " [-2.187382  ]\n",
      " [-3.4823387 ]\n",
      " [-1.1543914 ]\n",
      " [-2.187382  ]\n",
      " [-3.285744  ]\n",
      " [-0.6341317 ]\n",
      " [-1.378342  ]\n",
      " [-1.7582643 ]\n",
      " [-0.41905442]\n",
      " [-0.44303235]\n",
      " [-1.4531542 ]\n",
      " [-0.5946215 ]\n",
      " [-0.91487736]\n",
      " [-0.44303235]\n",
      " [-3.0606623 ]\n",
      " [-2.0665708 ]\n",
      " [-1.9603668 ]\n",
      " [-2.5582051 ]\n",
      " [-0.40363988]\n",
      " [-2.713493  ]\n",
      " [-2.713493  ]\n",
      " [-2.5582051 ]\n",
      " [-0.7970926 ]\n",
      " [-1.378342  ]\n",
      " [-2.9050164 ]\n",
      " [-1.6151656 ]\n",
      " [-0.2660063 ]\n",
      " [-1.5295323 ]\n",
      " [-1.6144718 ]\n",
      " [-1.2596596 ]\n",
      " [-0.46480182]\n",
      " [-0.2628551 ]\n",
      " [-1.174982  ]\n",
      " [-2.5717146 ]\n",
      " [-1.9977443 ]\n",
      " [-0.74662   ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -945.4127802162908\n",
      "Iteration 5\n",
      "Meta Reward: 29.873798722917172\n",
      "reward tf.Tensor(\n",
      "[[-3.403431  ]\n",
      " [-1.4520496 ]\n",
      " [-0.830201  ]\n",
      " [-1.4926505 ]\n",
      " [-1.9709057 ]\n",
      " [-2.9604392 ]\n",
      " [-0.830201  ]\n",
      " [-0.63179076]\n",
      " [-2.9410298 ]\n",
      " [-4.7877765 ]\n",
      " [-4.788994  ]\n",
      " [-4.3374653 ]\n",
      " [-1.063431  ]\n",
      " [-0.63179076]\n",
      " [-0.38690698]\n",
      " [-1.4926505 ]\n",
      " [-3.458597  ]\n",
      " [-2.9209247 ]\n",
      " [-0.8445854 ]\n",
      " [-3.5983434 ]\n",
      " [-4.787394  ]\n",
      " [-0.994405  ]\n",
      " [-2.3340316 ]\n",
      " [-2.1901484 ]\n",
      " [-2.5155003 ]\n",
      " [-0.66368294]\n",
      " [-4.6221385 ]\n",
      " [-4.314617  ]\n",
      " [-1.4926505 ]\n",
      " [-4.3374653 ]\n",
      " [-0.8065702 ]\n",
      " [-0.66195345]\n",
      " [-0.7734778 ]\n",
      " [-0.83805573]\n",
      " [-1.0338051 ]\n",
      " [-4.045277  ]\n",
      " [-1.7032375 ]\n",
      " [-4.788994  ]\n",
      " [-1.7822142 ]\n",
      " [-1.6205192 ]\n",
      " [-2.9604392 ]\n",
      " [-1.357038  ]\n",
      " [-1.4640808 ]\n",
      " [-4.314617  ]\n",
      " [-2.1242852 ]\n",
      " [-0.7734778 ]\n",
      " [-1.8253587 ]\n",
      " [-1.357038  ]\n",
      " [-0.8445854 ]\n",
      " [-0.6184275 ]\n",
      " [-2.1977222 ]\n",
      " [-3.9124386 ]\n",
      " [-2.955783  ]\n",
      " [-0.7734778 ]\n",
      " [-0.8445854 ]\n",
      " [-0.651748  ]\n",
      " [-1.2234038 ]\n",
      " [-0.7981811 ]\n",
      " [-2.524453  ]\n",
      " [-0.52361214]\n",
      " [-2.8950202 ]\n",
      " [-0.52361214]\n",
      " [-4.830816  ]\n",
      " [-1.7032375 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: -983.3647782437844\n",
      "Iteration 6\n",
      "Meta Reward: -163.65130392489732\n",
      "reward tf.Tensor(\n",
      "[[-2.7356384 ]\n",
      " [-3.3081372 ]\n",
      " [-2.238969  ]\n",
      " [-3.3617895 ]\n",
      " [-2.4929106 ]\n",
      " [-0.95581794]\n",
      " [-2.7356384 ]\n",
      " [-2.2652562 ]\n",
      " [-2.1153302 ]\n",
      " [-2.061025  ]\n",
      " [-2.185384  ]\n",
      " [-2.4253597 ]\n",
      " [-0.41974363]\n",
      " [-2.332492  ]\n",
      " [-3.342376  ]\n",
      " [-1.8895415 ]\n",
      " [-0.6484948 ]\n",
      " [-2.2143557 ]\n",
      " [-1.4794048 ]\n",
      " [-2.2103732 ]\n",
      " [-3.4706943 ]\n",
      " [-3.0697043 ]\n",
      " [-1.8733926 ]\n",
      " [-2.7356384 ]\n",
      " [-0.5956389 ]\n",
      " [-0.9062383 ]\n",
      " [-2.2652562 ]\n",
      " [-3.0697043 ]\n",
      " [-3.4422162 ]\n",
      " [-0.6305076 ]\n",
      " [-1.6679866 ]\n",
      " [-1.8895415 ]\n",
      " [-2.8175993 ]\n",
      " [-2.4929106 ]\n",
      " [-1.3950967 ]\n",
      " [-0.60321623]\n",
      " [-0.5899619 ]\n",
      " [-1.4900407 ]\n",
      " [-2.8817797 ]\n",
      " [-1.9692436 ]\n",
      " [-1.9798492 ]\n",
      " [-1.4218897 ]\n",
      " [-1.0401504 ]\n",
      " [-1.1968865 ]\n",
      " [-2.061025  ]\n",
      " [-2.2143557 ]\n",
      " [-0.6484948 ]\n",
      " [-1.5012944 ]\n",
      " [-0.7063565 ]\n",
      " [-2.2652562 ]\n",
      " [-2.4249856 ]\n",
      " [-0.5899619 ]\n",
      " [-2.1378946 ]\n",
      " [-2.4249856 ]\n",
      " [-3.1342366 ]\n",
      " [-2.4249856 ]\n",
      " [-1.6394047 ]\n",
      " [-2.1043737 ]\n",
      " [-1.1444567 ]\n",
      " [-1.9848548 ]\n",
      " [-0.687836  ]\n",
      " [-1.5012944 ]\n",
      " [-1.9798492 ]\n",
      " [-1.9100205 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -978.8426376672952\n",
      "Iteration 7\n",
      "Meta Reward: -22.46622152296345\n",
      "reward tf.Tensor(\n",
      "[[-1.1216934 ]\n",
      " [-0.7676271 ]\n",
      " [-0.68934774]\n",
      " [-1.2892822 ]\n",
      " [-1.8591392 ]\n",
      " [-0.93578166]\n",
      " [-1.0613301 ]\n",
      " [-1.9104478 ]\n",
      " [-0.758649  ]\n",
      " [-0.6856748 ]\n",
      " [-0.95214045]\n",
      " [-1.7540978 ]\n",
      " [-1.3805857 ]\n",
      " [-0.95214045]\n",
      " [-1.1411314 ]\n",
      " [-1.8853552 ]\n",
      " [-0.9150473 ]\n",
      " [-1.0059369 ]\n",
      " [-1.0064821 ]\n",
      " [-1.3179183 ]\n",
      " [-0.6511422 ]\n",
      " [-0.9274613 ]\n",
      " [-0.8507354 ]\n",
      " [-0.6722218 ]\n",
      " [-0.7212681 ]\n",
      " [-1.3805857 ]\n",
      " [-0.6751323 ]\n",
      " [-1.573094  ]\n",
      " [-0.7672936 ]\n",
      " [-1.1508496 ]\n",
      " [-1.59624   ]\n",
      " [-1.1576638 ]\n",
      " [-1.7379516 ]\n",
      " [-0.94423443]\n",
      " [-0.7676271 ]\n",
      " [-0.6751323 ]\n",
      " [-1.8430344 ]\n",
      " [-0.7187932 ]\n",
      " [-1.7603709 ]\n",
      " [-1.3805857 ]\n",
      " [-1.0613301 ]\n",
      " [-0.9274613 ]\n",
      " [-1.1378074 ]\n",
      " [-1.278615  ]\n",
      " [-0.941298  ]\n",
      " [-1.3363276 ]\n",
      " [-1.3114995 ]\n",
      " [-0.68934774]\n",
      " [-1.9398618 ]\n",
      " [-1.0064821 ]\n",
      " [-0.7187932 ]\n",
      " [-0.8507354 ]\n",
      " [-1.3007249 ]\n",
      " [-1.9104478 ]\n",
      " [-1.9310926 ]\n",
      " [-1.7540978 ]\n",
      " [-1.2455546 ]\n",
      " [-1.2032776 ]\n",
      " [-0.8467256 ]\n",
      " [-1.1508496 ]\n",
      " [-1.2892822 ]\n",
      " [-1.3114995 ]\n",
      " [-1.1605812 ]\n",
      " [-1.944094  ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -989.8657988629784\n",
      "Iteration 8\n",
      "Meta Reward: 279.52042175209203\n",
      "reward tf.Tensor(\n",
      "[[-1.6043674 ]\n",
      " [-3.012746  ]\n",
      " [-4.7581906 ]\n",
      " [-0.85278076]\n",
      " [-4.5715322 ]\n",
      " [-1.0842551 ]\n",
      " [-4.653297  ]\n",
      " [-3.912854  ]\n",
      " [-3.6142852 ]\n",
      " [-4.8682556 ]\n",
      " [-3.6142852 ]\n",
      " [-4.9624095 ]\n",
      " [-3.2559683 ]\n",
      " [-1.0381454 ]\n",
      " [-4.942869  ]\n",
      " [-1.0381454 ]\n",
      " [-1.8053035 ]\n",
      " [-3.1246912 ]\n",
      " [-4.759148  ]\n",
      " [-4.5510645 ]\n",
      " [-0.8319591 ]\n",
      " [-4.0483727 ]\n",
      " [-2.7380574 ]\n",
      " [-2.522098  ]\n",
      " [-3.7770855 ]\n",
      " [-1.8053035 ]\n",
      " [-2.8461738 ]\n",
      " [-2.4061394 ]\n",
      " [-1.2435516 ]\n",
      " [-2.7431362 ]\n",
      " [-0.60205525]\n",
      " [-4.9624095 ]\n",
      " [-1.0381454 ]\n",
      " [-2.1459177 ]\n",
      " [-2.1459177 ]\n",
      " [-3.5015037 ]\n",
      " [-3.1931427 ]\n",
      " [-4.8229356 ]\n",
      " [-4.1352916 ]\n",
      " [-2.7431362 ]\n",
      " [-3.2819693 ]\n",
      " [-2.580926  ]\n",
      " [-3.1931427 ]\n",
      " [-2.7431362 ]\n",
      " [-5.0009255 ]\n",
      " [-0.8472172 ]\n",
      " [-1.3743416 ]\n",
      " [-0.60205525]\n",
      " [-0.72578615]\n",
      " [-2.538695  ]\n",
      " [-1.398055  ]\n",
      " [-3.1246912 ]\n",
      " [-3.7770855 ]\n",
      " [-4.494408  ]\n",
      " [-4.653297  ]\n",
      " [-4.412748  ]\n",
      " [-2.244219  ]\n",
      " [-3.9385192 ]\n",
      " [-4.9624095 ]\n",
      " [-2.7110627 ]\n",
      " [-2.8938792 ]\n",
      " [-1.8053035 ]\n",
      " [-2.8288405 ]\n",
      " [-2.580926  ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1042.5913897360222\n",
      "Iteration 9\n",
      "Meta Reward: -340.02543220417215\n",
      "reward tf.Tensor(\n",
      "[[-3.7512352]\n",
      " [-2.7645144]\n",
      " [-2.1871555]\n",
      " [-1.740864 ]\n",
      " [-1.150446 ]\n",
      " [-1.2809025]\n",
      " [-1.740864 ]\n",
      " [-0.9489815]\n",
      " [-2.412381 ]\n",
      " [-2.0851986]\n",
      " [-1.6413256]\n",
      " [-3.7374952]\n",
      " [-2.0263827]\n",
      " [-3.8748012]\n",
      " [-0.9156431]\n",
      " [-1.740864 ]\n",
      " [-3.4121938]\n",
      " [-3.7541683]\n",
      " [-1.7187086]\n",
      " [-1.0483655]\n",
      " [-3.9069955]\n",
      " [-1.6448575]\n",
      " [-1.5007058]\n",
      " [-3.4920788]\n",
      " [-2.3001251]\n",
      " [-1.5007058]\n",
      " [-2.518746 ]\n",
      " [-2.6225984]\n",
      " [-0.8207334]\n",
      " [-3.145919 ]\n",
      " [-1.7035409]\n",
      " [-2.524478 ]\n",
      " [-1.1843804]\n",
      " [-2.6225984]\n",
      " [-1.2809025]\n",
      " [-2.6359196]\n",
      " [-3.145919 ]\n",
      " [-3.75767  ]\n",
      " [-3.556579 ]\n",
      " [-3.145919 ]\n",
      " [-2.6918588]\n",
      " [-2.3001251]\n",
      " [-2.518746 ]\n",
      " [-1.6583639]\n",
      " [-3.5258071]\n",
      " [-2.6585464]\n",
      " [-0.9109669]\n",
      " [-3.7907372]\n",
      " [-1.8245316]\n",
      " [-0.9156431]\n",
      " [-2.524478 ]\n",
      " [-3.7512352]\n",
      " [-1.1764978]\n",
      " [-1.1843804]\n",
      " [-3.2054672]\n",
      " [-0.9489815]\n",
      " [-2.6225984]\n",
      " [-2.6918588]\n",
      " [-3.4121938]\n",
      " [-2.4625106]\n",
      " [-0.9489815]\n",
      " [-2.6483994]\n",
      " [-3.8688414]\n",
      " [-2.6585464]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1117.945930142983\n",
      "Iteration 10\n",
      "Meta Reward: 881.9400156985697\n",
      "reward tf.Tensor(\n",
      "[[-0.8823334]\n",
      " [-6.544479 ]\n",
      " [-6.58931  ]\n",
      " [-5.08869  ]\n",
      " [-5.1148124]\n",
      " [-4.951429 ]\n",
      " [-5.005432 ]\n",
      " [-4.4636655]\n",
      " [-4.1550856]\n",
      " [-7.220395 ]\n",
      " [-1.4691747]\n",
      " [-1.4691747]\n",
      " [-2.0870004]\n",
      " [-2.7159455]\n",
      " [-2.2878423]\n",
      " [-7.6565814]\n",
      " [-4.4523344]\n",
      " [-5.0904274]\n",
      " [-7.855682 ]\n",
      " [-4.147631 ]\n",
      " [-7.220395 ]\n",
      " [-2.7159455]\n",
      " [-1.5716463]\n",
      " [-4.397528 ]\n",
      " [-5.793635 ]\n",
      " [-2.0870004]\n",
      " [-2.397345 ]\n",
      " [-7.3223524]\n",
      " [-2.2036965]\n",
      " [-5.2568846]\n",
      " [-7.63633  ]\n",
      " [-2.9717429]\n",
      " [-2.3954787]\n",
      " [-3.0524204]\n",
      " [-4.095941 ]\n",
      " [-1.3253124]\n",
      " [-0.8823334]\n",
      " [-5.1850305]\n",
      " [-1.4693716]\n",
      " [-2.397345 ]\n",
      " [-4.314513 ]\n",
      " [-1.3340667]\n",
      " [-7.78308  ]\n",
      " [-2.7121758]\n",
      " [-2.3954787]\n",
      " [-4.951429 ]\n",
      " [-3.8903646]\n",
      " [-5.22043  ]\n",
      " [-3.8903646]\n",
      " [-6.1237597]\n",
      " [-3.3676097]\n",
      " [-2.8206937]\n",
      " [-2.2878423]\n",
      " [-2.7159455]\n",
      " [-4.583472 ]\n",
      " [-4.095941 ]\n",
      " [-6.544479 ]\n",
      " [-1.4625258]\n",
      " [-3.3558664]\n",
      " [-5.1148124]\n",
      " [-5.1148124]\n",
      " [-1.5077152]\n",
      " [-7.3223524]\n",
      " [-3.3676097]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n",
      "Avg Reward: -1132.6369659853183\n",
      "Iteration 11\n",
      "Meta Reward: 307.1699953220965\n",
      "reward tf.Tensor(\n",
      "[[-6.226518  ]\n",
      " [-6.1979094 ]\n",
      " [-4.957999  ]\n",
      " [-4.71065   ]\n",
      " [-8.735447  ]\n",
      " [-5.198148  ]\n",
      " [-9.094038  ]\n",
      " [-3.416085  ]\n",
      " [-8.116756  ]\n",
      " [-1.4837955 ]\n",
      " [-2.511896  ]\n",
      " [-3.620097  ]\n",
      " [-6.1966867 ]\n",
      " [-6.1979094 ]\n",
      " [-8.105812  ]\n",
      " [-5.971672  ]\n",
      " [-5.6171865 ]\n",
      " [-1.9228792 ]\n",
      " [-4.957999  ]\n",
      " [-3.4159858 ]\n",
      " [-3.3579056 ]\n",
      " [-9.130389  ]\n",
      " [-7.266548  ]\n",
      " [-5.4929295 ]\n",
      " [-3.3579056 ]\n",
      " [-4.1133795 ]\n",
      " [-2.511896  ]\n",
      " [-4.5180254 ]\n",
      " [-5.4929295 ]\n",
      " [-6.0511923 ]\n",
      " [-4.924979  ]\n",
      " [-1.9228792 ]\n",
      " [-1.6370845 ]\n",
      " [-8.398534  ]\n",
      " [-6.1979094 ]\n",
      " [-1.1173403 ]\n",
      " [-1.1081643 ]\n",
      " [-8.853102  ]\n",
      " [-1.6370845 ]\n",
      " [-6.2624    ]\n",
      " [-5.6171865 ]\n",
      " [-1.3449963 ]\n",
      " [-1.4444226 ]\n",
      " [-4.924979  ]\n",
      " [-1.947541  ]\n",
      " [-1.1173403 ]\n",
      " [-1.2251809 ]\n",
      " [-2.126509  ]\n",
      " [-1.3484291 ]\n",
      " [-1.4789605 ]\n",
      " [-3.4159858 ]\n",
      " [-0.93594676]\n",
      " [-5.4628963 ]\n",
      " [-3.2269847 ]\n",
      " [-2.1402826 ]\n",
      " [-3.1295912 ]\n",
      " [-3.4159858 ]\n",
      " [-1.948698  ]\n",
      " [-3.9231784 ]\n",
      " [-9.130389  ]\n",
      " [-1.3484291 ]\n",
      " [-8.116756  ]\n",
      " [-5.613902  ]\n",
      " [-8.953029  ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: -1161.5047834857285\n",
      "Iteration 12\n",
      "Meta Reward: 211.34793431246385\n",
      "reward tf.Tensor(\n",
      "[[-9.77233  ]\n",
      " [-1.513365 ]\n",
      " [-5.083912 ]\n",
      " [-9.20497  ]\n",
      " [-5.846279 ]\n",
      " [-3.8491254]\n",
      " [-1.2743717]\n",
      " [-1.4431977]\n",
      " [-3.773774 ]\n",
      " [-1.5481465]\n",
      " [-7.8233614]\n",
      " [-9.77233  ]\n",
      " [-2.1798074]\n",
      " [-9.823645 ]\n",
      " [-9.521228 ]\n",
      " [-6.07767  ]\n",
      " [-6.7480597]\n",
      " [-5.5281734]\n",
      " [-5.981125 ]\n",
      " [-6.08379  ]\n",
      " [-2.9470441]\n",
      " [-2.8121605]\n",
      " [-4.4294715]\n",
      " [-8.906838 ]\n",
      " [-4.9170446]\n",
      " [-5.083912 ]\n",
      " [-6.5841265]\n",
      " [-2.0493639]\n",
      " [-3.773774 ]\n",
      " [-4.4294715]\n",
      " [-7.215905 ]\n",
      " [-3.3491886]\n",
      " [-3.4265864]\n",
      " [-9.018871 ]\n",
      " [-8.906838 ]\n",
      " [-1.7234373]\n",
      " [-5.24605  ]\n",
      " [-2.63984  ]\n",
      " [-9.018871 ]\n",
      " [-6.7414827]\n",
      " [-7.000994 ]\n",
      " [-5.846279 ]\n",
      " [-9.764033 ]\n",
      " [-8.04685  ]\n",
      " [-4.6088223]\n",
      " [-7.4147296]\n",
      " [-9.823645 ]\n",
      " [-3.2782824]\n",
      " [-9.784657 ]\n",
      " [-1.0951868]\n",
      " [-9.20497  ]\n",
      " [-6.662958 ]\n",
      " [-9.917292 ]\n",
      " [-9.917292 ]\n",
      " [-1.471551 ]\n",
      " [-5.3120265]\n",
      " [-2.537707 ]\n",
      " [-8.219901 ]\n",
      " [-8.906838 ]\n",
      " [-6.4214883]\n",
      " [-5.3742666]\n",
      " [-8.04685  ]\n",
      " [-5.846279 ]\n",
      " [-9.203794 ]], shape=(64, 1), dtype=float32)\n",
      "log_reward tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(64, 1), dtype=float32)\n",
      "Teacher Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bbf2fcfcf996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Step 10:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mddpg_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdata_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md1_rollout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmeta_reward_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-453d0cd94cbf>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(policy, n)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_torque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_torque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m  \u001b[0;31m# for rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mangle_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mthdot\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mnewthdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthdot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 4:\n",
    "for it in range(total_iterations):\n",
    "    print('Iteration', it)\n",
    "    # Step 5: Generate d0\n",
    "    data, _ = generate(teacher_actor, d0_rollout)\n",
    "    d0.append(data)\n",
    "    \n",
    "    # Step 6: Update pi to temp\n",
    "    # ********* ddpg_update的optimizer用 actor_optimizer (不知道會不會出事)\n",
    "    temp = copy(actor)\n",
    "    ddpg_update(d0, temp, actor_optimizer, critic, critic_optimizer, update_critic=False)\n",
    "    \n",
    "    # Step 7: \n",
    "    data, meta_pip = generate(temp, d1_rollout)\n",
    "    d1.append(data)\n",
    "    meta = meta_pip - meta_pi\n",
    "    print('Meta Reward:', meta)\n",
    "    \n",
    "    # Step 8:\n",
    "    loss = teacher_update(teacher_actor, teacher_actor_optim, teacher_critic, teacher_critic_optim, d0, meta)\n",
    "    print('Teacher Loss:', loss)\n",
    "    teacher_loss_list.append(loss)\n",
    "\n",
    "    # Step 9:\n",
    "    buffer.append(d0.raw)\n",
    "    buffer.append(d1.raw)\n",
    "    \n",
    "    # Step 10:\n",
    "    ddpg_update(buffer, actor, actor_optimizer, critic, critic_optimizer)\n",
    "    data_temp, meta_pi = generate(actor, d1_rollout)\n",
    "    \n",
    "    meta_reward_list.append(meta)\n",
    "    it_reward_list.append(meta_pi)\n",
    "    avg_reward = np.mean(it_reward_list[-40:])\n",
    "    print(\"Avg Reward:\", avg_reward)\n",
    "    avg_reward_list.append(avg_reward)\n",
    "    \n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Avg. Iteration Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(meta_reward_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Meta Iteration Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(teacher_loss_list)\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Teacher Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
