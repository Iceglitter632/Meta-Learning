{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def insert(self, d):\n",
    "        self.buffer.append(d)\n",
    "        over = self.size - self.capacity\n",
    "        self.buffer = self.buffer[over:]\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    @property\n",
    "    def batches(self):\n",
    "        state_batch = tf.convert_to_tensor([self.buffer[i][0] for i in range(len(a))])\n",
    "        action_batch = tf.convert_to_tensor([self.buffer[i][1] for i in range(len(a))])\n",
    "        reward_batch = tf.convert_to_tensor([self.buffer[i][2] for i in range(len(a))])\n",
    "        next_state_batch = tf.convert_to_tensor([self.buffer[i][3] for i in range(len(a))])\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "\n",
    "@tf.function\n",
    "def ddpg_update(d, actor, actor_optimizer, critic, critic_optimizer, update_critic=True):\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = d.batches()\n",
    "    \n",
    "    if update_critic:\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = critic([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actions = actor(state_batch, training=True)\n",
    "        critic_value = critic([state_batch, actions], training=True)\n",
    "        # Used `-value` as we want to maximize the value given\n",
    "        # by the critic for our actions\n",
    "        actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "    actor_grad = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def target_update(target, model):\n",
    "    target.variables = model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_update(teacher, teacher_optimizer, d, meta):\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = d.batches()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        log_reward = tf.math.log(reward_batch)\n",
    "        loss = meta * tf.math.reduce_sum(log_reward)\n",
    "    \n",
    "    teacher_grad = tape.gradient(loss, teacher.trainable_variables)\n",
    "    teacher_optimizer.apply_gradients(zip(teacher_grad, teacher.trainable_variables))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give: policy(model) and current state\n",
    "# return: next action\n",
    "def next_action(policy, state):\n",
    "    state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "    \n",
    "    sampled_actions = tf.squeeze(policy(state))\n",
    "    sampled_actions = sampled_actions.numpy()\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n",
    "# take 1 step and update prev_state, episodic_reward for you\n",
    "# give: action\n",
    "# return : 'sars': (prev_state, action, reward, state), 'done': done\n",
    "episodic_reward = 0\n",
    "ep_reward_list = []  # To store reward history of each episode\n",
    "avg_reward_list = [] # To store average reward history of last few episodes\n",
    "def env_step(action):\n",
    "    state, reward, done, info = env.step(action)\n",
    "    episodic_reward += reward\n",
    "    \n",
    "    if done:    \n",
    "        prev_state = env.reset()\n",
    "        ep_reward_list.append(episodic_reward)\n",
    "        episodic_reward = 0\n",
    "        # Mean of last 40 episodes\n",
    "        avg_reward = np.mean(ep_reward_list[-40:])\n",
    "        print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        \n",
    "    \n",
    "    sp = prev_state\n",
    "    prev_state = state\n",
    "    return {'sars': (sp, action, reward, state), 'done': done}\n",
    "\n",
    "# sample n steps using policy\n",
    "# give: policy, n\n",
    "# return: [sars], accumulated reward\n",
    "def generate(policy, n):\n",
    "    #宗叡把 env.reset()搬到這邊\n",
    "    prev_state = env.reset()\n",
    "    d = []\n",
    "    reward = 0\n",
    "    for i in range(n):\n",
    "        action = next_action(policy, prev_state)\n",
    "        \n",
    "        ##### 宗叡寫的\n",
    "        \n",
    "        s, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        d.append((prev_state, action, r, s))\n",
    "        prev_state = s\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        ###### 宗叡寫的結束\n",
    "        \n",
    "#         sars, done = env_step(action)\n",
    "#         reward += sars[2]\n",
    "#         prev_state = sars[3]\n",
    "#         d.append(sars)\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "    return d, reward\n",
    "\n",
    "def copy(model, optimizer):\n",
    "#     new_model = tf.keras.models.clone_model(model)\n",
    "#     new_model.set_weights(model.get_weights())\n",
    "    \n",
    "    new_model = get_actor()\n",
    "    new_model.variables = model.variable\n",
    "    \n",
    "    new_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "    grad_vars = model.trainable_weights\n",
    "    zero_grads = [tf.zeros_like(w) for w in grad_vars]\n",
    "    new_optimizer.apply_gradients(zip(zero_grads, grad_vars))\n",
    "    new_optimizer.set_weights(optimizer.get_weights())\n",
    "    \n",
    "    return new_model, new_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "teacher_lr = 0.0001\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "\n",
    "total_iterations = 10000\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "d0_rollout = 100\n",
    "d1_rollout = 200\n",
    "pi_rollout = 50\n",
    "buffer_capacity = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 0: Define variables\n",
    "# 此行註解為宗叡註解，如果要使用在用回來就好\n",
    "# prev_state = env.reset()\n",
    "\n",
    "# Step 1: Initialize pi\n",
    "teacher = get_actor()\n",
    "\n",
    "# Step 1: Initialize pie\n",
    "actor = get_actor()\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "critic = get_critic()\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "# Making the weights equal initially\n",
    "teacher.set_weights(actor.get_weights())\n",
    "teacher_optim = tf.keras.optimizers.Adam(teacher_lr)\n",
    "\n",
    "# Step 2:\n",
    "d0, d1 = Buffer(d0_rollout), Buffer(d1_rollout)\n",
    "d1.buffer, meta_pi = generate(actor, d1_rollout)\n",
    "buffer = Buffer(buffer_capacity)\n",
    "buffer.insert(d1.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-845b6a96b805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Step 6: Update pi to temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mddpg_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-7704a7962e09>\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(model, optimizer)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mnew_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mnew_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'variable'"
     ]
    }
   ],
   "source": [
    "# Step 4:\n",
    "for it in range(total_iterations):\n",
    "    # Step 5: Generate d0\n",
    "    d0.buffer, _ = generate(teacher, d0_rollout)\n",
    "    \n",
    "    # Step 6: Update pi to temp\n",
    "    temp, temp_optimizer = copy(actor, actor_optimizer)\n",
    "    ddpg_update(d0, temp, temp_optimizer, critic, critic_optimizer)\n",
    "    \n",
    "    # Step 7: \n",
    "    d1.buffer, meta_pip = generate(temp, d1_rollout)\n",
    "    meta = meta_pip - meta_pi\n",
    "    \n",
    "    # Step 8:\n",
    "    teacher_update(teacher, teacher_optimizer, d0, meta)\n",
    "    \n",
    "    # Step 9:\n",
    "    buffer.insert(d0.buffer)\n",
    "    buffer.insert(d1.buffer)\n",
    "    \n",
    "    # Step 10:\n",
    "    ddpg_update(buffer, actor, actor_optimizer, critic, critic_optimizer)\n",
    "    \n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
